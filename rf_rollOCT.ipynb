{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0f14adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.ensemble import BaseEnsemble\n",
    "from scipy.stats import mode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "827adbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling_lookahead_dt_pulp import rollo_oct_pulp\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# was hiermit eben nicht geht ist, dass man auf Trainingsdaten trainiert (was einem das reine Modell geben sollte). Dabei werden aber leider gleichzeitig\n",
    "# die Testdaten auf diesen Modell predicted\n",
    "# Das Resultat ist also, dass man nicht andere Testdaten auf dem fertigen modell testen kann\n",
    "\n",
    "\n",
    "\n",
    "class CustomTreeWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, depth=None, criterion='gini', test_data=None, target_label=None, features=None):\n",
    "        self.depth = depth\n",
    "        self.criterion = criterion\n",
    "        self.test_data = test_data\n",
    "        self.target_label = target_label\n",
    "        self.features = features\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.test_data is None or self.target_label is None or self.features is None:\n",
    "            raise ValueError(\"Test data, target label, and features must be set before calling fit\")\n",
    "        \n",
    "        # Construct train DataFrame or array compatible with rollo_oct_pulp.run interface\n",
    "        train_df = pd.concat([y, X], axis=1, ignore_index=False)\n",
    "        #print(train_df)\n",
    "        # Call your custom run with both train and stored test data\n",
    "        self.result_dict_ = rollo_oct_pulp.run(\n",
    "            train=train_df,\n",
    "            test=self.test_data,\n",
    "            target_label=self.target_label,\n",
    "            features=self.features,\n",
    "            depth=self.depth,\n",
    "            criterion=self.criterion\n",
    "        )\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "\n",
    "        res = self.result_dict_['tree'][self.depth]['test']\n",
    "        #check  = res.iloc[:, :1]\n",
    "        #check_series = check.squeeze(\"columns\") #convert to series\n",
    "\n",
    "        #test = pd.DataFrame.equals(X, check_series)\n",
    "\n",
    "        #if test:\n",
    "        preds = res['prediction']\n",
    "        if preds is None:\n",
    "            raise RuntimeError(\"No stored predictions found. Run fit first.\")\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1e744f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CustomRandomForest(BaseEnsemble, ClassifierMixin):\\n    def __init__(self, base_estimator=None, n_estimators=10, max_depth=None, random_state=None):\\n        super().__init__(\\n            base_estimator=base_estimator,\\n            n_estimators=n_estimators,\\n            random_state=random_state,\\n        )\\n        self.max_depth = max_depth\\n\\n    def _validate_estimators(self):\\n        # This ensures base_estimator is cloned n_estimators times\\n        super()._validate_estimators()\\n        if self.base_estimator is None:\\n            self.base_estimator = CustomTreeClassifier(max_depth=self.max_depth)\\n\\n    def fit(self, X, y):\\n        self._validate_estimators()\\n        self.estimators_ = []\\n        n_samples = X.shape[0]\\n        rng = np.random.RandomState(self.random_state)\\n\\n        for i in range(self.n_estimators):\\n            # Bootstrap sample indices\\n            indices = rng.choice(n_samples, size=n_samples, replace=True)\\n            X_sample, y_sample = X[indices], y[indices]\\n            estimator = clone(self.base_estimator)\\n            estimator.fit(X_sample, y_sample)\\n            self.estimators_.append(estimator)\\n        return self\\n\\n    def predict(self, X):\\n        # Gather predictions from all trees\\n        predictions = np.array([tree.predict(X) for tree in self.estimators_])\\n        maj_vote, _ = mode(predictions, axis=0)\\n        return maj_vote.ravel()\\n        '"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class CustomRandomForest(BaseEnsemble, ClassifierMixin):\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, max_depth=None, random_state=None):\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _validate_estimators(self):\n",
    "        # This ensures base_estimator is cloned n_estimators times\n",
    "        super()._validate_estimators()\n",
    "        if self.base_estimator is None:\n",
    "            self.base_estimator = CustomTreeClassifier(max_depth=self.max_depth)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._validate_estimators()\n",
    "        self.estimators_ = []\n",
    "        n_samples = X.shape[0]\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sample indices\n",
    "            indices = rng.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_sample, y_sample = X[indices], y[indices]\n",
    "            estimator = clone(self.base_estimator)\n",
    "            estimator.fit(X_sample, y_sample)\n",
    "            self.estimators_.append(estimator)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Gather predictions from all trees\n",
    "        predictions = np.array([tree.predict(X) for tree in self.estimators_])\n",
    "        maj_vote, _ = mode(predictions, axis=0)\n",
    "        return maj_vote.ravel()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a93f3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRandomForest(BaseEnsemble, ClassifierMixin):\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, max_depth=None, \n",
    "                 random_state=None, test_data=None, target_label=None, features=None, criterion='gini'):\n",
    "        super().__init__(n_estimators=n_estimators)\n",
    "        self.random_state = random_state  # we store this ourselves\n",
    "        self.base_estimator = base_estimator\n",
    "        self.max_depth = max_depth\n",
    "        self.test_data = test_data\n",
    "        self.target_label = target_label\n",
    "        self.features = features\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def _validate_estimators(self):\n",
    "        super()._validate_estimators()\n",
    "        # Provide default base_estimator if none given\n",
    "        if self.base_estimator is None:\n",
    "            self.base_estimator = CustomTreeWrapper(\n",
    "                depth=self.max_depth,\n",
    "                criterion=self.criterion,\n",
    "                test_data=self.test_data,\n",
    "                target_label=self.target_label,\n",
    "                features=self.features,\n",
    "            )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._validate_estimators()\n",
    "        self.estimators_ = []\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sample indices\n",
    "            indices = rng.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_sample, y_sample = X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "            # Clone base estimator to get fresh copy\n",
    "            estimator = clone(self.base_estimator)\n",
    "            estimator.fit(X_sample, y_sample)\n",
    "            self.estimators_.append(estimator)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "        \n",
    "        # Collect predictions from each tree\n",
    "        predictions = np.array([tree.predict(X) for tree in self.estimators_])\n",
    "        \n",
    "        # Majority vote along first axis (trees)\n",
    "        maj_vote, _ = mode(predictions, axis=0)\n",
    "        return maj_vote.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8f14aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_seismic = pd.read_csv(\"datasets/seismic/seismic_bin.csv\")\n",
    "\n",
    "\n",
    "X = data_seismic.drop(columns=['y'])  # All columns except the target\n",
    "y = data_seismic['y']                 # Only the target column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, stratify=y, random_state=42)\n",
    "\n",
    "stacked_train = pd.concat([y_train, X_train], axis=1, ignore_index=False)\n",
    "stacked_test = pd.concat([y_test, X_test],axis=1, ignore_index=False)\n",
    "\n",
    "train_data = stacked_train\n",
    "test_data = stacked_test\n",
    "\n",
    "#feature_columns = train_data.columns[1:]\n",
    "\n",
    "feature_columns = X_train.columns\n",
    "#print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9445631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Usage outside:\\nwrapper = CustomTreeWrapper(\\n    depth=3,\\n    criterion='gini',\\n    test_data=test_data,\\n    target_label='y',\\n    features=feature_columns\\n)\\nwrapper.fit(X_train, y_train); #semicolon for suppressing when interactive environment (Jupyter) tries to display the returned object, suppresses repr(self)\\n\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Usage outside:\n",
    "wrapper = CustomTreeWrapper(\n",
    "    depth=3,\n",
    "    criterion='gini',\n",
    "    test_data=test_data,\n",
    "    target_label='y',\n",
    "    features=feature_columns\n",
    ")\n",
    "wrapper.fit(X_train, y_train); #semicolon for suppressing when interactive environment (Jupyter) tries to display the returned object, suppresses repr(self)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3675b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = wrapper.predict(X_test) #X_test must be the same that was used to create test_data; otherwise the logic of the wrapper breaks and will not give correct result\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9dc510a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '_validate_estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m forest = CustomRandomForest(\n\u001b[32m      3\u001b[39m     n_estimators=\u001b[32m5\u001b[39m,\n\u001b[32m      4\u001b[39m     max_depth=\u001b[32m3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     criterion=\u001b[33m'\u001b[39m\u001b[33mgini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Fit on train data only - internally, each base estimator will use complete test data from constructor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mforest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Predict on test data (must be the same test_data-frame used)\u001b[39;00m\n\u001b[32m     15\u001b[39m forest_preds = forest.predict(test_data[feature_columns])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mCustomRandomForest.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_estimators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mself\u001b[39m.estimators_ = []\n\u001b[32m     28\u001b[39m     rng = np.random.RandomState(\u001b[38;5;28mself\u001b[39m.random_state)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mCustomRandomForest._validate_estimators\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_estimators\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_estimators\u001b[49m()\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Provide default base_estimator if none given\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'super' object has no attribute '_validate_estimators'"
     ]
    }
   ],
   "source": [
    "# Instantiate custom random forest using same test data fixed at construction\n",
    "forest = CustomRandomForest(\n",
    "    n_estimators=5,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    test_data=test_data,\n",
    "    target_label='y',\n",
    "    features=feature_columns,\n",
    "    criterion='gini',\n",
    ")\n",
    "# Fit on train data only - internally, each base estimator will use complete test data from constructor\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data (must be the same test_data-frame used)\n",
    "forest_preds = forest.predict(test_data[feature_columns])\n",
    "print(\"Random forest predictions:\", forest_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
