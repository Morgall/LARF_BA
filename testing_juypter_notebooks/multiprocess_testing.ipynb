{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669eaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Go up one directory to get to master/\n",
    "project_root = str(Path.cwd().parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from helpers.helpers import preprocess_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2840163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(arr: np.array,\n",
    "               instance_size: int,\n",
    "               K: list,\n",
    "               y_idx: int = 0,\n",
    "               weighted: bool = True):\n",
    "    \"\"\"\n",
    "\n",
    "    :param arr:\n",
    "    :param instance_size:\n",
    "    :param K:\n",
    "    :param y_idx:\n",
    "    :param weighted:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sum_ = 0\n",
    "    for k in K:\n",
    "        sum_ += np.power(len(arr[np.where(arr[:, y_idx] == k)]) / len(arr), 2)\n",
    "    sum_ = 1 - sum_\n",
    "    if weighted:\n",
    "        sum_ = (len(arr) / instance_size) * sum_\n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accd88c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(data: pd.DataFrame,\n",
    "                   P: list,\n",
    "                   K: list,\n",
    "                   nodes: dict) -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    :param data:\n",
    "    :param P:\n",
    "    :param K:\n",
    "    :param nodes:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_arr = np.array(data)\n",
    "    n = len(data)\n",
    "    gini_dict = dict()\n",
    "    for leaf_ in nodes[\"leaf_nodes\"]:\n",
    "        temp = dict()\n",
    "        first_var = nodes[\"leaf_nodes_path\"][leaf_][0]\n",
    "        second_var = nodes[\"leaf_nodes_path\"][leaf_][1]\n",
    "        for feature_i in P:\n",
    "            arr = df_arr[np.where((df_arr[:, feature_i] == first_var))]\n",
    "\n",
    "            for feature_j in P:\n",
    "                arr_2 = arr[np.where(arr[:, feature_j] == second_var)]\n",
    "                if len(arr_2) > 0:\n",
    "                    temp[feature_i, feature_j] = gini_index(arr=arr_2,\n",
    "                                                            instance_size=n,\n",
    "                                                            K=K,\n",
    "                                                            weighted=True)\n",
    "        gini_dict[leaf_] = copy.deepcopy(temp)\n",
    "        del temp\n",
    "        del arr\n",
    "\n",
    "    return gini_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "483f0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling_lookahead_dt_pulp.oct.tree import generate_nodes\n",
    "from pulp import *\n",
    "\n",
    "def generate_model_pulp(\n",
    "        P: list,\n",
    "        K: list,\n",
    "        data: pd.DataFrame,\n",
    "        y_idx: int = 0,\n",
    "        time_limit: float = 1800, # moved to train_model to give parameter to solver directly\n",
    "        gap_limit: float = None,\n",
    "        log_to_console: bool = False,\n",
    "        big_m: int = 99,\n",
    "        criterion: str = \"gini\",\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param criterion:\n",
    "    :param big_m:\n",
    "    :param depth:\n",
    "    :param P:\n",
    "    :param K:\n",
    "    :param data:\n",
    "    :param leaf_nodes_path:\n",
    "    :param y_idx:\n",
    "    :param time_limit:\n",
    "    :param gap_limit:\n",
    "    :param log_to_console:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create parent & leaf nodes\n",
    "\n",
    "    leaf_nodes_path = {4: [1, 1], #dict for all leafes in depth 2 tree and der respective split conditions\n",
    "                       5: [1, 0],\n",
    "                       6: [0, 1],\n",
    "                       7: [0, 0]}\n",
    "    depth = 2\n",
    "    parent_nodes, leaf_nodes = generate_nodes(depth) # (for depth 2) returns [1,2,3] and [4,5,6,7]\n",
    "\n",
    "    nodes = dict()\n",
    "    nodes[\"leaf_nodes\"] = leaf_nodes\n",
    "    nodes[\"leaf_nodes_path\"] = leaf_nodes_path\n",
    "\n",
    "    print(nodes)\n",
    "\n",
    "    if criterion == \"gini\":\n",
    "        logging.info(\"Calculating gini..\")\n",
    "        coef_dict = calculate_gini(data=data,\n",
    "                                   P=P,\n",
    "                                   K=K,\n",
    "                                   nodes=nodes)\n",
    "        \n",
    "    elif criterion == \"misclassification\":\n",
    "        logging.info(\"Calculating misclassification..\")\n",
    "        coef_dict = calculate_misclassification(data=data,\n",
    "                                                P=P,\n",
    "                                                nodes=nodes)\n",
    "    # init model\n",
    "    model = LpProblem(\"RollOCT\", LpMinimize) # Sets the objective to be minimized\n",
    "\n",
    "    # # x[i,j] and y[i,k] as binary variables\n",
    "    x = dict()\n",
    "    for i in P:\n",
    "        for j in P:\n",
    "            x[i, j] = LpVariable(f'x[{i},{j}]', cat='Binary') #creates variable\n",
    "            # In PuLP, variables are automatically added to the model when you include them in constraints or the objective function—you do not need to explicitly register them with the model\n",
    "            # cat='Binary': Specifies the variable type as binary\n",
    "            # name=f'x[{i}, {j}]': Assigns a name to the variable for easier identification and debugging\n",
    "            # x[i, j]: Binary variable indicating whether feature i is used for the first split and feature j for the second split\n",
    "\n",
    "    y = dict()\n",
    "    for i in P:\n",
    "        for k in P:\n",
    "            y[i, k] = LpVariable(f'y[{i},{k}]', cat='Binary')\n",
    "\n",
    "#lpSum is to gulps what quicksum is to gurobi\n",
    "\n",
    "    # Constraint 1: Exactly one (i,j) pair is selected\n",
    "    model += lpSum(x[i,j] for i in P for j in P) == 1, \"C1b\" # Ensures that exactly one combination of features (i, j) is selected for the first two splits (criteria (1b))\n",
    "    # Constraint 2: Exactly one (i,k) pair is selected\n",
    "    model += lpSum(y[i,k] for i in P for k in P) == 1, \"C1c\"  # implements criteria (1c) same way as above\n",
    "    # Constraint 3\n",
    "    for i in P:\n",
    "        model += lpSum(x[i,j] for j in P) == lpSum(y[i,k] for k in P), f\"C1d_{i}\" # Links the x and y variables, ensuring that for each feature i, the sum of x[i, j] across j equals the sum of y[i, k] across k. This ensures consistency between the splits (criteria (1d))\n",
    "\n",
    "    # Constraint 4\n",
    "    # add big m; Acts as a penalty for invalid splits (if a combination is not present in coef_dict, it uses big_m as a large penalty)\n",
    "\n",
    "# implements criteria (1a)\n",
    "# variable x[i, j], y[i, k] is binary, so 1 or 0. So the multiplication makes sense\n",
    "# .get(key, default): Dictionary method to safely retrieve values, using big_m as a fallback\n",
    "# coef_dict[4].get((i, j))\n",
    "# in coef_dict[4].get((i, j) is value of loss function of leaf 4 with features i,j from P. Value is found in dict if it matches the respective split condition of leaf 4 [1, 1]. If not penalty big m is used\n",
    "# => damit der Wert fuer loss function bei feature combination i,j im coef_dict steht, müssen i,j (nach definition wie coef_dict erstellt wird) dort zur split condition (vorhanden/nicht vorhanden) gematched haben  \n",
    "# => also man kann in coef_dict[4].get((i, j, big_m) nur Werte für die features finden, die im Datensatz der Kombination [1,1] entsprochen haben. Falls der Wert nicht vorhanden ist (alle i,j Kombinationen werden iteriert), wird stattdessen Wert big_m genommen\n",
    "\n",
    "    obj = lpSum(\n",
    "        (coef_dict[4].get((i, j), big_m) + coef_dict[5].get((i, j), big_m)) *\n",
    "        x[i, j]\n",
    "        for i in P\n",
    "        for j in P) + \\\n",
    "          lpSum((coef_dict[6].get((i, k), big_m) + coef_dict[7].get((i, k), big_m)) *\n",
    "                   y[i, k]\n",
    "                   for i in P\n",
    "                   for k in P)\n",
    "\n",
    "    model += obj, \"Objective\"\n",
    "\n",
    "\n",
    "    # #set a time limit (if using a compatible solver like CBC or Gurobi):\n",
    "    # if time_limit:\n",
    "    #     m.setParam(\"TimeLimit\", time_limit)\n",
    "    #     logging.info(f'Setting Time Limit as {time_limit}')\n",
    "\n",
    "    # if gap_limit is not None:\n",
    "    #     m.setParam(\"MipGap\", gap_limit) #MipGap: Sets the optimality gap tolerance for early termination\n",
    "    #     logging.info(f'Setting Optimality Gap as {gap_limit}')\n",
    "\n",
    "    # m.setParam(\"LogToConsole\", int(log_to_console))\n",
    "    # logging.info(f'Setting LogToConsole as {log_to_console}')\n",
    "    # m.update() #Updates the model with all changes.\n",
    "\n",
    "    model_dict = {\n",
    "        'model': model,\n",
    "        'params': {\n",
    "            'var_x': x,\n",
    "            'var_y': y,\n",
    "            'y_idx': y_idx\n",
    "        },\n",
    "        'nodes': {\n",
    "            'leaf_nodes': leaf_nodes,\n",
    "            'parent_nodes': parent_nodes,\n",
    "            \"leaf_nodes_path\": leaf_nodes_path,\n",
    "        },\n",
    "        'depth': depth,\n",
    "        \"P\": P,\n",
    "        \"K\": K\n",
    "    }\n",
    "    logging.info('Model generation is done.')\n",
    "\n",
    "    return model_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leaf_nodes': [4, 5, 6, 7], 'leaf_nodes_path': {4: [1, 1], 5: [1, 0], 6: [0, 1], 7: [0, 0]}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Parameter aus dem Aufrug von run\n",
    "    depth = 8\n",
    "    time_limit = 1800\n",
    "    criterion = \"gini\"\n",
    "\n",
    "    dataset_name = 'test'\n",
    "    if dataset_name == 'adult'\n",
    "    # Load your training and test datasets\n",
    "        data = pd.read_csv(\"data/stacked.csv\")\n",
    "\n",
    "    if dataset_name == 'adult'\n",
    "    # Load your training and test datasets\n",
    "        data = pd.read_csv(\"data/adult/stacked.csv\")\n",
    "    \n",
    "    if dataset_name == 'adult'\n",
    "    # Load your training and test datasets\n",
    "        data = pd.read_csv(\"data/adult/stacked.csv\")\n",
    "\n",
    "    X = data.drop(columns=['y'])  # All columns except the target\n",
    "    y = data['y']                 # Only the target column\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, stratify=y, random_state=42)\n",
    "\n",
    "    stacked_train = pd.concat([y_train, X_train], axis=1, ignore_index=False)\n",
    "    stacked_test = pd.concat([y_test, X_test],axis=1, ignore_index=False)\n",
    "\n",
    "    train_data = stacked_train\n",
    "    test_data = stacked_test\n",
    "\n",
    "    feature_columns = train_data.columns[1:]\n",
    "\n",
    "    train, test = preprocess_dataframes( #./rollo_oct/utils/helpers.py\n",
    "        train_df=train_data,\n",
    "        test_df=test_data,\n",
    "        target_label=\"y\",\n",
    "        features=feature_columns)\n",
    "\n",
    "    df = pd.concat([train, test])\n",
    "    P = [int(i) for i in\n",
    "         list(train.loc[:, train.columns != 'y'].columns)]\n",
    "    train.columns = [\"y\", *P]\n",
    "    test.columns = [\"y\", *P]\n",
    "    K = sorted(list(set(df.y)))\n",
    "\n",
    "    main_model = generate_model_pulp(P=P, K=K, data=train, y_idx=0, big_m=99, criterion=criterion)\n",
    "    print(main_model)\n",
    "\n",
    "    # time default auf adult: 4min51.1s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
