{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669eaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cProfile\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Go up one directory to get to master/\n",
    "project_root = str(Path.cwd().parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from helpers.helpers import preprocess_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2840163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(arr: np.array,\n",
    "               instance_size: int,\n",
    "               K: list,\n",
    "               y_idx: int = 0,\n",
    "               weighted: bool = True):\n",
    "    \"\"\"\n",
    "\n",
    "    :param arr: The subset of data to evaluate (rows = instances, columns = features/labels).\n",
    "    :param instance_size: The total number of instances in the original dataset (used for weighting)\n",
    "    :param K: list — The list of unique class labels\n",
    "    :param y_idx: y_idx: int (default 0) — The index of the column in arr that contains the class labels.\n",
    "    :param weighted: Whether to weight the Gini index by the proportion of the subset size to the total dataset size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sum_ = 0\n",
    "    for k in K:\n",
    "        sum_ += np.power(len(arr[np.where(arr[:, y_idx] == k)]) / len(arr), 2) # arr[np.where(arr[:, y_idx] == k)] selects all rows where the class label is k, len(...) / len(arr) computes the proportion of class k in the subset\n",
    "    sum_ = 1 - sum_\n",
    "    if weighted:\n",
    "        sum_ = (len(arr) / instance_size) * sum_\n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accd88c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini_old(data: pd.DataFrame,\n",
    "                   P: list,\n",
    "                   K: list,\n",
    "                   nodes: dict) -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    :param data: pd.DataFrame — The dataset\n",
    "    :param P: list Indices of features to consider\n",
    "    :param K: lsit Unique class labels\n",
    "    :param nodes: dict — Contains: \"leaf_nodes\": list of leaf node identifiers. leaf_nodes_path\": dict mapping each leaf node to a path (list of values for features).\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_arr = np.array(data)\n",
    "    n = len(data) # total number of instances.\n",
    "    gini_dict = dict()\n",
    "    for leaf_ in nodes[\"leaf_nodes\"]:\n",
    "        temp = dict() # store Gini values for this leaf\n",
    "        first_var = nodes[\"leaf_nodes_path\"][leaf_][0] #{'leaf_nodes': [4, 5, 6, 7], 'leaf_nodes_path': {4: [1, 1], 5: [1, 0], 6: [0, 1], 7: [0, 0]}}\n",
    "        second_var = nodes[\"leaf_nodes_path\"][leaf_][1] #first_var, second_var — the first two values in the path to this leaf (used as feature values for filtering)\n",
    "        for feature_i in P:\n",
    "            arr = df_arr[np.where((df_arr[:, feature_i] == first_var))] #Selects rows where feature feature_i equals first_var.\n",
    "\n",
    "            for feature_j in P:\n",
    "                arr_2 = arr[np.where(arr[:, feature_j] == second_var)] #Further filters to rows where feature feature_j equals second_var\n",
    "                if len(arr_2) > 0: #Calculate Gini index for arr_2 (so for all rows matching (1, 0; having/not having feature i) the decision variables)\n",
    "                    temp[feature_i, feature_j] = gini_index(arr=arr_2,\n",
    "                                                            instance_size=n,\n",
    "                                                            K=K,\n",
    "                                                            weighted=True)\n",
    "        gini_dict[leaf_] = copy.deepcopy(temp)\n",
    "        del temp\n",
    "        del arr\n",
    "\n",
    "    return gini_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990df2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index_fast(y):\n",
    "    \"\"\"Vectorized Gini calculation for a 1D array of labels.\"\"\"\n",
    "    _, counts = np.unique(y, return_counts=True) #returns the unique labels in y and how many times each appears, counts is an array of the number of occurrences for each unique label\n",
    "    probs = counts / len(y)\n",
    "    return 1 - np.sum(probs ** 2)\n",
    "    #probs = counts / counts.sum()\n",
    "    #if counts.sum() == len(y):\n",
    "        #return 1 - np.sum(probs ** 2)\n",
    "\n",
    "def calculate_gini_fast(data: pd.DataFrame, P: list, K: list, nodes: dict) -> dict:\n",
    "    \"\"\"\n",
    "    :param data: pd.DataFrame — The dataset\n",
    "    :param P: list of Indices of features to consider\n",
    "    :param K: list Unique class labels\n",
    "    :param nodes: dict — Contains: \"leaf_nodes\": list of leaf node identifiers. leaf_nodes_path\": dict mapping each leaf node to a path (list of values for features).\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gini_dict = dict()\n",
    "    n = len(data) #Total number of rows in the dataset\n",
    "    for leaf_ in nodes[\"leaf_nodes\"]:\n",
    "        temp = dict()\n",
    "        #These are the feature values that define the path to this leaf (e.g., in a decision tree, the values that must be matched to reach this leaf):\n",
    "        first_var = nodes[\"leaf_nodes_path\"][leaf_][0] #{'leaf_nodes': [4, 5, 6, 7], 'leaf_nodes_path': {4: [1, 1], 5: [1, 0], 6: [0, 1], 7: [0, 0]}}\n",
    "        second_var = nodes[\"leaf_nodes_path\"][leaf_][1] #first_var, second_var — the first two values in the path to this leaf (used as feature values for filtering)\n",
    "        for feature_i in P:\n",
    "            arr = data[data.iloc[:, feature_i] == first_var] # filters to rows where feature_i equals first_var\n",
    "            for feature_j in P:\n",
    "                arr_2 = arr[arr.iloc[:, feature_j] == second_var] #Further filters arr to rows where feature_j equals second_var\n",
    "                if not arr_2.empty:\n",
    "                    gini = gini_index_fast(arr_2.iloc[:, 0].values) #Calculates the Gini impurity of the labels in the first column (assumed to be the label column).\n",
    "                    weighted_gini = (len(arr_2) / n) * gini\n",
    "                    temp[(feature_i, feature_j)] = weighted_gini\n",
    "        gini_dict[leaf_] = temp\n",
    "    return gini_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22371068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_for_leaf(args):\n",
    "    leaf_, df_arr, n, P, K, nodes = args\n",
    "    temp = dict()\n",
    "    first_var = nodes[\"leaf_nodes_path\"][leaf_][0]\n",
    "    second_var = nodes[\"leaf_nodes_path\"][leaf_][1]\n",
    "    for feature_i in P:\n",
    "        arr = df_arr[np.where((df_arr[:, feature_i] == first_var))]\n",
    "        for feature_j in P:\n",
    "            arr_2 = arr[np.where(arr[:, feature_j] == second_var)]\n",
    "            if len(arr_2) > 0:\n",
    "                temp[feature_i, feature_j] = gini_index(\n",
    "                    arr=arr_2, instance_size=n, K=K, weighted=True\n",
    "                )\n",
    "    return (leaf_, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "761c5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def calculate_gini(data: pd.DataFrame, P: list, K: list, nodes: dict, amount_cores = 1) -> dict:\n",
    "    df_arr = np.array(data)\n",
    "    n = len(data)\n",
    "    # Prepare arguments for each process\n",
    "    args = [\n",
    "        (leaf_, df_arr, n, P, K, nodes)\n",
    "        for leaf_ in nodes[\"leaf_nodes\"]\n",
    "    ]\n",
    "    with Pool(processes=amount_cores) as pool:\n",
    "        results = pool.map(gini_for_leaf, args)\n",
    "    # Combine results into a dictionary\n",
    "    gini_dict = {leaf_: temp for leaf_, temp in results}\n",
    "    return gini_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "483f0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling_lookahead_dt_pulp.oct.tree import generate_nodes\n",
    "from pulp import *\n",
    "\n",
    "def generate_model_pulp(\n",
    "        P: list,\n",
    "        K: list,\n",
    "        data: pd.DataFrame,\n",
    "        y_idx: int = 0,\n",
    "        time_limit: float = 1800, # moved to train_model to give parameter to solver directly\n",
    "        gap_limit: float = None,\n",
    "        log_to_console: bool = False,\n",
    "        big_m: int = 99,\n",
    "        amount_cores = 1,\n",
    "        test = False,\n",
    "        criterion: str = \"gini\",\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param criterion:\n",
    "    :param big_m:\n",
    "    :param depth:\n",
    "    :param P:\n",
    "    :param K:\n",
    "    :param data:\n",
    "    :param leaf_nodes_path:\n",
    "    :param y_idx:\n",
    "    :param time_limit:\n",
    "    :param gap_limit:\n",
    "    :param log_to_console:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create parent & leaf nodes\n",
    "\n",
    "    leaf_nodes_path = {4: [1, 1], #dict for all leafes in depth 2 tree and der respective split conditions\n",
    "                       5: [1, 0],\n",
    "                       6: [0, 1],\n",
    "                       7: [0, 0]}\n",
    "    depth = 2\n",
    "    parent_nodes, leaf_nodes = generate_nodes(depth) # (for depth 2) returns [1,2,3] and [4,5,6,7]\n",
    "\n",
    "    nodes = dict()\n",
    "    nodes[\"leaf_nodes\"] = leaf_nodes\n",
    "    nodes[\"leaf_nodes_path\"] = leaf_nodes_path\n",
    "\n",
    "    print(nodes)\n",
    "\n",
    "    if criterion == \"gini\":\n",
    "        logging.info(\"Calculating gini..\")\n",
    "        #coef_dict = calculate_gini(data=data, P=P, K=K, nodes=nodes, amount_cores = amount_cores)\n",
    "        if test == False:\n",
    "            coef_dict = calculate_gini_fast(data=data, P=P, K=K, nodes=nodes)\n",
    "            #with open(f'./result_gini_fast.txt', 'w') as f:\n",
    "                #f.write(str(coef_dict))\n",
    "        else:\n",
    "            coef_dict = calculate_gini_old(data=data, P=P, K=K, nodes=nodes)\n",
    "            #with open(f'./result_gini_old.txt', 'w') as f:\n",
    "                #f.write(str(coef_dict))\n",
    "\n",
    "        \n",
    "    elif criterion == \"misclassification\":\n",
    "        logging.info(\"Calculating misclassification..\")\n",
    "        coef_dict = calculate_misclassification(data=data,\n",
    "                                                P=P,\n",
    "                                                nodes=nodes)\n",
    "    # init model\n",
    "    model = LpProblem(\"RollOCT\", LpMinimize) # Sets the objective to be minimized\n",
    "\n",
    "    # # x[i,j] and y[i,k] as binary variables\n",
    "    x = dict()\n",
    "    for i in P:\n",
    "        for j in P:\n",
    "            x[i, j] = LpVariable(f'x[{i},{j}]', cat='Binary') #creates variable\n",
    "            # In PuLP, variables are automatically added to the model when you include them in constraints or the objective function—you do not need to explicitly register them with the model\n",
    "            # cat='Binary': Specifies the variable type as binary\n",
    "            # name=f'x[{i}, {j}]': Assigns a name to the variable for easier identification and debugging\n",
    "            # x[i, j]: Binary variable indicating whether feature i is used for the first split and feature j for the second split\n",
    "\n",
    "    y = dict()\n",
    "    for i in P:\n",
    "        for k in P:\n",
    "            y[i, k] = LpVariable(f'y[{i},{k}]', cat='Binary')\n",
    "\n",
    "#lpSum is to gulps what quicksum is to gurobi\n",
    "\n",
    "    # Constraint 1: Exactly one (i,j) pair is selected\n",
    "    model += lpSum(x[i,j] for i in P for j in P) == 1, \"C1b\" # Ensures that exactly one combination of features (i, j) is selected for the first two splits (criteria (1b))\n",
    "    # Constraint 2: Exactly one (i,k) pair is selected\n",
    "    model += lpSum(y[i,k] for i in P for k in P) == 1, \"C1c\"  # implements criteria (1c) same way as above\n",
    "    # Constraint 3\n",
    "    for i in P:\n",
    "        model += lpSum(x[i,j] for j in P) == lpSum(y[i,k] for k in P), f\"C1d_{i}\" # Links the x and y variables, ensuring that for each feature i, the sum of x[i, j] across j equals the sum of y[i, k] across k. This ensures consistency between the splits (criteria (1d))\n",
    "\n",
    "    # Constraint 4\n",
    "    # add big m; Acts as a penalty for invalid splits (if a combination is not present in coef_dict, it uses big_m as a large penalty)\n",
    "\n",
    "# implements criteria (1a)\n",
    "# variable x[i, j], y[i, k] is binary, so 1 or 0. So the multiplication makes sense\n",
    "# .get(key, default): Dictionary method to safely retrieve values, using big_m as a fallback\n",
    "# coef_dict[4].get((i, j))\n",
    "# in coef_dict[4].get((i, j) is value of loss function of leaf 4 with features i,j from P. Value is found in dict if it matches the respective split condition of leaf 4 [1, 1]. If not penalty big m is used\n",
    "# => damit der Wert fuer loss function bei feature combination i,j im coef_dict steht, müssen i,j (nach definition wie coef_dict erstellt wird) dort zur split condition (vorhanden/nicht vorhanden) gematched haben  \n",
    "# => also man kann in coef_dict[4].get((i, j, big_m) nur Werte für die features finden, die im Datensatz der Kombination [1,1] entsprochen haben. Falls der Wert nicht vorhanden ist (alle i,j Kombinationen werden iteriert), wird stattdessen Wert big_m genommen\n",
    "\n",
    "    obj = lpSum(\n",
    "        (coef_dict[4].get((i, j), big_m) + coef_dict[5].get((i, j), big_m)) *\n",
    "        x[i, j]\n",
    "        for i in P\n",
    "        for j in P) + \\\n",
    "          lpSum((coef_dict[6].get((i, k), big_m) + coef_dict[7].get((i, k), big_m)) *\n",
    "                   y[i, k]\n",
    "                   for i in P\n",
    "                   for k in P)\n",
    "\n",
    "    model += obj, \"Objective\"\n",
    "\n",
    "\n",
    "    # #set a time limit (if using a compatible solver like CBC or Gurobi):\n",
    "    # if time_limit:\n",
    "    #     m.setParam(\"TimeLimit\", time_limit)\n",
    "    #     logging.info(f'Setting Time Limit as {time_limit}')\n",
    "\n",
    "    # if gap_limit is not None:\n",
    "    #     m.setParam(\"MipGap\", gap_limit) #MipGap: Sets the optimality gap tolerance for early termination\n",
    "    #     logging.info(f'Setting Optimality Gap as {gap_limit}')\n",
    "\n",
    "    # m.setParam(\"LogToConsole\", int(log_to_console))\n",
    "    # logging.info(f'Setting LogToConsole as {log_to_console}')\n",
    "    # m.update() #Updates the model with all changes.\n",
    "\n",
    "    model_dict = {\n",
    "        'model': model,\n",
    "        'params': {\n",
    "            'var_x': x,\n",
    "            'var_y': y,\n",
    "            'y_idx': y_idx\n",
    "        },\n",
    "        'nodes': {\n",
    "            'leaf_nodes': leaf_nodes,\n",
    "            'parent_nodes': parent_nodes,\n",
    "            \"leaf_nodes_path\": leaf_nodes_path,\n",
    "        },\n",
    "        'depth': depth,\n",
    "        \"P\": P,\n",
    "        \"K\": K\n",
    "    }\n",
    "    logging.info('Model generation is done.')\n",
    "\n",
    "    return model_dict , coef_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leaf_nodes': [4, 5, 6, 7], 'leaf_nodes_path': {4: [1, 1], 5: [1, 0], 6: [0, 1], 7: [0, 0]}}\n"
     ]
    }
   ],
   "source": [
    "from deepdiff import DeepDiff\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameter aus dem Aufrug von run\n",
    "    depth = 8\n",
    "    time_limit = 1800\n",
    "    criterion = \"gini\"\n",
    "    amount_cores = 7\n",
    "\n",
    "    dataset_name = 'adult'\n",
    "\n",
    "    if dataset_name == 'test':\n",
    "    # Load your training and test datasets\n",
    "        data = pd.read_csv(\"data/stacked.csv\")\n",
    "\n",
    "    if dataset_name == 'adult':\n",
    "    # Load your training and test datasets\n",
    "        data = pd.read_csv(\"data/adult/stacked.csv\")\n",
    "\n",
    "    X = data.drop(columns=['y'])  # All columns except the target\n",
    "    y = data['y']                 # Only the target column\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, stratify=y, random_state=42)\n",
    "\n",
    "    stacked_train = pd.concat([y_train, X_train], axis=1, ignore_index=False)\n",
    "    stacked_test = pd.concat([y_test, X_test],axis=1, ignore_index=False)\n",
    "\n",
    "    train_data = stacked_train\n",
    "    test_data = stacked_test\n",
    "\n",
    "    feature_columns = train_data.columns[1:]\n",
    "\n",
    "    train, test = preprocess_dataframes( #./rollo_oct/utils/helpers.py\n",
    "        train_df=train_data,\n",
    "        test_df=test_data,\n",
    "        target_label=\"y\",\n",
    "        features=feature_columns)\n",
    "\n",
    "    df = pd.concat([train, test])\n",
    "    P = [int(i) for i in\n",
    "         list(train.loc[:, train.columns != 'y'].columns)]\n",
    "    train.columns = [\"y\", *P]\n",
    "    test.columns = [\"y\", *P]\n",
    "    K = sorted(list(set(df.y)))\n",
    "\n",
    "    #profiler = cProfile.Profile()\n",
    "    #profiler.enable()\n",
    "    #main_model_fast, coef_dict_fast = generate_model_pulp(P=P, K=K, data=train, y_idx=0, big_m=99, criterion=criterion, amount_cores = amount_cores)\n",
    "    main_model_old, coef_dict_old = generate_model_pulp(P=P, K=K, data=train, y_idx=0, big_m=99, criterion=criterion, amount_cores = amount_cores, test=True)\n",
    "    #profiler.disable()\n",
    "    #profiler.dump_stats(\"profile_gini_preprocess.out\")\n",
    "\n",
    "    #print(coef_dict_fast == coef_dict_old)\n",
    "    #diff = DeepDiff(coef_dict_fast, coef_dict_old)\n",
    "    #print(diff)\n",
    "\n",
    "    #gini_fast: 3m5.2s\n",
    "    #gini_old: 4m46.9s\n",
    "\n",
    "    # time default adult: 4min51.1s\n",
    "    # time multithreaded adult: 4m 27.8s, 4m21.3s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
