{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b98154d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Go up one directory to get to master/\n",
    "project_root = str(Path.cwd().parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from rolling_lookahead_dt_pulp import rollo_oct_pulp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17995149",
   "metadata": {},
   "source": [
    "there are options, e.g.:\n",
    "\n",
    "\n",
    "# Binarizer: Converts numeric features to binary (0/1) based on a threshold.\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "\n",
    "X_binary = binarizer.fit_transform(X)\n",
    "\n",
    "\n",
    "# OneHotEncoder: Converts categorical features to binary (one-hot encoding).\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "X_binary = enc.fit_transform(X_categorical)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3439aa",
   "metadata": {},
   "source": [
    "# for which kind of data doesnt make_data_binary work?\n",
    "\n",
    "The make_data_binary function as written is not suitable for all types of data. Here are cases where it may fail or be inappropriate:\n",
    "\n",
    "Text Data with Multiple Tokens or Lists:\n",
    "If your DataFrame contains columns with lists of strings (e.g., ['one', 'two', 'three']), or multi-term entries, the function will not properly transform them into binary features. It expects each cell to be a single value, not a list or multi-valued string.\n",
    "\n",
    "Ordinal Categorical Data:\n",
    "The function treats all non-binary categorical columns as nominal (no order), and applies one-hot encoding. If your data has ordinal categories (e.g., 'low', 'medium', 'high'), this encoding discards the order information, which may not be optimal for modeling.\n",
    "\n",
    "High-Cardinality Categorical Data:\n",
    "If a categorical column has a very large number of unique values, one-hot encoding will create many new columns, leading to high dimensionality and computational inefficiency.\n",
    "\n",
    "Continuous Numerical Data:\n",
    "The function does not handle continuous numerical features (like age, income) well, as it will one-hot encode them if they have more than two unique values, which is almost never desired for continuous variables.\n",
    "\n",
    "Missing Values in Target Column:\n",
    "The function does not explicitly handle missing values in the 'y' column, only in features. If 'y' has missing values, the function may fail or produce unexpected results.\n",
    "\n",
    "Non-Standard Data Types:\n",
    "If your DataFrame contains columns with complex data types (e.g., dictionaries, custom objects), the function will not process them correctly.\n",
    "\n",
    "Columns with Non-Binary Unique Values but Not Categorical:\n",
    "If a column has exactly two unique values but is not meant to be treated as binary (e.g., a continuous variable with only two unique values due to sampling), the function will still treat it as binary, which may not be appropriate.\n",
    "\n",
    "In summary:\n",
    "The function works best for DataFrames with simple categorical and binary features, with a single, well-defined target column. It is not suitable for text with multiple terms, ordinal data, high-cardinality categoricals, continuous numerical features, complex data types, or missing values in the target column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3b3b1",
   "metadata": {},
   "source": [
    "# Example Workflow\n",
    "\n",
    "Input: DataFrame with mixed types, some missing values, and a target column 'y'.\n",
    "\n",
    "Output: DataFrame with all features binary/one-hot encoded, 'y' as the first column, and columns renamed.\n",
    "\n",
    "## Key Points\n",
    "\n",
    "Missing values are handled by mode imputation.\n",
    "\n",
    "Binary columns are ensured to be 0 and 1.\n",
    "\n",
    "Non-binary columns are one-hot encoded.\n",
    "\n",
    "The target column 'y' is made numerical and moved to the front.\n",
    "\n",
    "Columns are renamed for consistency.\n",
    "\n",
    "Logging is used throughout for transparency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d62d70",
   "metadata": {},
   "source": [
    "# Was man beachten muss:\n",
    "\n",
    "needs target label column to be named 'y' => aus dem Datensatz immer erstmal dieses Spaltenlabel umbenennen\n",
    "\n",
    "Es wird kompliziert, wenn die Target labels (jetzt 'y') im Datensatz nicht in der ersten Spalte stehen.\n",
    "make_data_binary platziert y in der ersten Spalte und benennt alle features in 1,2,3... um. \n",
    "Das macht spätere zuordnung schwieriger bzw. man muss halt darauf achten\n",
    "\n",
    "=> es ist also vll. schlau jeden Datensatz vor make_data_binary erstmal so zu prozessieren, dass Target label in erster Spalte steht. Zusätzlich kann man hier dann auch das umbenennen der Target label Spalte direkt machen => also evtl einfach das als helper Funktion implementieren\n",
    "=> dann ergibt auch das spätere\n",
    "\n",
    "#get features\n",
    "feature_columns = train_data.columns[1:] #assuming labels are in first column\n",
    "\n",
    "entsprechend Sinn\n",
    "\n",
    "das ist dann zwar redundanter code, aber kostet keine Zeit\n",
    "Generell gibts von denen auch einigen redundanten code. Zum Teil doppelt sich das was das preprocessing tut/prüft mit dem was make_data_binary tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0950f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# muss dann in helpers.py kopiert werden\n",
    "\n",
    "# moves cloumn with target labels to first column and renames it to 'y' and return reordered dataframe\n",
    "def move_targets_to_front_and_rename(data: pd.DataFrame, target_label='y') -> pd.DataFrame:\n",
    "    data.rename(columns={target_label: 'y'}, inplace=True)\n",
    "    if data.columns[0] != 'y': # Checks if 'y' is not the first column; hier wurde vorher aus irgendeinem Grund data.columns[-1] != \"y\" geprüft, also ob 'y' in letzter Spalte war\n",
    "        logging.info(\"Reordering y column at the beginning of data.\")\n",
    "        cols_ = list(data.columns)\n",
    "        cols_.remove('y')\n",
    "        cols_.insert(0, 'y')\n",
    "        data = data[cols_]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c45bbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function transforms input data (a pandas DataFrame) into a binary/one-hot encoded format suitable for certain machine learning tasks.\n",
    "# It handles missing values, binary columns, categorical columns, and the target column separately.\n",
    "# needs target label column to be named 'y'\n",
    "\n",
    "def make_data_binary(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    :param data: input data\n",
    "    :return: data with binary columns\n",
    "    \"\"\"\n",
    "    cols_with_missing = [col for col in data.columns #Identify Columns with Missing Values\n",
    "                         if data[col].isnull().any()]\n",
    "    if cols_with_missing:\n",
    "        for col in cols_with_missing:\n",
    "            data[col].fillna(data[col].mode()[0], inplace=True) # Replace Missing Values with Mode: For each column with missing values, fill them with the most frequent value (mode).\n",
    "        logging.info(\"\"\"There are columns with missing\n",
    "            values.\\nColumns are: {0}\\n Replacing with mode. \n",
    "            \"\"\".format(cols_with_missing))\n",
    "    \n",
    "    \n",
    "\n",
    "    binary_cols = [cname for cname in data.columns if # Find Columns with Exactly 2 Unique Values (excluding 'y');  list of column names that have exactly two unique values (excluding 'y')\n",
    "                   data[cname].nunique() == 2 and cname != 'y']\n",
    "\n",
    "    for col in binary_cols: # Convert Non-Integer Binary Columns to Integer: If a binary column is not already an integer type, convert it using category codes.\n",
    "        if data[col].dtype not in ['int8', 'int16']: # check data type\n",
    "            logging.info(f\"Column {col} is not int type. Transforming it into \"\n",
    "                         f\"integer.\")\n",
    "            data[col] = data[col].astype('category').cat.codes\n",
    "            # astype('category'): Converts the column to a pandas \"category\" type for categorical variables (integer codes; cat.codes) (limited, fixed set of possible values). This is useful for columns with a small number of unique values.\n",
    "\n",
    "    # Ensure Binary Columns are 0 and 1: If the unique values do not sum to 1 (i.e., not already 0 and 1), remap them to 0 and 1.\n",
    "    if binary_cols: \n",
    "        for col in binary_cols:\n",
    "            # if sum of unique entries is not equal to 1\n",
    "            if sum(data[col].unique()) != 1:\n",
    "                replace = {data[col].unique()[0]: 0, # Create a mapping dictionary where the first unique value maps to 0 and the second to 1. Apply this mapping to the entire column, converting all values to 0 or 1.\n",
    "                           data[col].unique()[1]: 1} # Example: If the column has values ['A', 'B'], it will be mapped to {'A': 0, 'B': 1}\n",
    "                data[col] = [replace[item] for item in data[col]]\n",
    "        logging.info(\"There are {0} binary columns. \\nColumns are: {1}\".format(\n",
    "            len(binary_cols), binary_cols))\n",
    "    else:\n",
    "        logging.info(\"No binary columns.\")\n",
    "        # so now it is ensured that binary columns contain strict binary (0/1) encoding\n",
    "\n",
    "\n",
    "    total_col = 0 #expected total number of columns after all transformations; total number of columns after one-hot encoding and other transformations\n",
    "\n",
    "\n",
    "    # Log the Number of Unique Values for Each Non-Target Column\n",
    "    # non-binary columns (excluding 'y'), sum the number of unique values\n",
    "    # This is used to check if one-hot encoding later produces the expected number of columns.\n",
    "    for col in data.columns:\n",
    "        if col != \"y\":\n",
    "            logging.info(\"Column: {0} - Unique Values: {1}\".format( #For each non-target column, log its name and the number of unique values it contains.\n",
    "                col, data[col].nunique()))\n",
    "            if data[col].nunique() != 2:\n",
    "                total_col += data[col].nunique() #if a column does NOT have exactly 2 unique values (i.e., it’s not a binary column), add its number of unique values to total_col.\n",
    "    total_col += len(binary_cols) + 1\n",
    "    # This is preparation for one-hot encoding:\n",
    "    # For categorical columns, one-hot encoding will create as many new columns as there are unique values.\n",
    "    # For binary columns, no extra columns are needed beyond the original (since they are already handled separately).\n",
    "\n",
    "\n",
    "    #the one hot encoding\n",
    "    for col in data.columns:\n",
    "        if col not in binary_cols and col != \"y\": #for Each Non-Binary, Non-Target Column:\n",
    "            dummy_col = pd.get_dummies(data[col], prefix=col,dtype=int) #Creates a new DataFrame (dummy_col) where the original column is split into multiple binary (0/1) columns, one for each unique value in the original column\n",
    "            data = pd.concat([data, dummy_col], axis=1) # adds new dummy columns to the original DataFrame\n",
    "            data = data.drop(col, axis=1) # Removes the original column from the DataFrame, since it got replaced by one-hot encoding\n",
    "\n",
    "    if total_col != data.shape[1]: # If the expected number of columns does not match the actual, log an error and return None\n",
    "        logging.error(\"# of expected column is not equal to actual.\")\n",
    "        return None\n",
    "\n",
    "    # Convert Target Column 'y' to Categorical Codes\n",
    "    if data.y.dtype == \"O\": # checks if the column 'y' is of type object (usually strings or mixed types)\n",
    "        logging.info(\"Converting y values into numerical.\")\n",
    "        data['y'] = data['y'].astype('category') # converts the column to a categorical type\n",
    "        data['y'] = data['y'].cat.codes # replaces each unique value in 'y' with a numerical code (e.g., \"cat\" → 0, \"dog\" → 1, etc.)\n",
    "\n",
    "    # ensure Target Values Are At Least 1, if not shift all values up by 1\n",
    "    if data.y.min() < 1:\n",
    "        data['y'] = data['y'] + 1\n",
    "    # Reason: Some algorithms or libraries expect labels to start at 1 rather than 0\n",
    "\n",
    "    # move Target Column 'y' to the Front\n",
    "    if data.columns[0] != \"y\": # Checks if 'y' is not the first column; hier wurde vorher aus irgendeinem Grund data.columns[-1] != \"y\" geprüft, also ob 'y' in letzter Spalte war\n",
    "        logging.info(\"Reordering y column at the beginning of data.\")\n",
    "        cols_ = list(data.columns)\n",
    "        cols_.remove('y')\n",
    "        cols_.insert(0, \"y\")\n",
    "        data = data[cols_]\n",
    "\n",
    "    # Rename All Columns Except 'y' to 1, 2, 3, ..., based on their position\n",
    "    logging.info(\"Renaming columns..\")\n",
    "    column_indices = [i for i in range(1, len(data.columns))]\n",
    "    new_names = column_indices\n",
    "    old_names = data.columns[column_indices]\n",
    "    data.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    logging.info(\"Data is binarized.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2fbb7",
   "metadata": {},
   "source": [
    "# Implementierung für preprocessing numerical features\n",
    "following approach in paper:\n",
    "\"For numerical features, we follow a two-step approach. If the number of unique values observed for a feature is less than 7, then it is treated as categorical. Otherwise, it is discretized into equal-sized groups based on sample quantiles.\"\n",
    "\n",
    "After this one hot encoding is used on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8bac604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of preprocessing for \"For numerical features, we follow a two-step approach. If the number of unique values observed for a feature is less than 7, then it is treated as categorical. Otherwise, it is discretized into equal-sized groups based on sample quantiles.\"\n",
    "\n",
    "def preprocess_numerical(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    binary_cols = [cname for cname in data.columns if data[cname].nunique() == 2] # Find Columns with Exactly 2 Unique Values; list of column names that have exactly two unique values\n",
    "    int_cols_non_binary = [col for col in data.select_dtypes(include=['int']).columns \n",
    "                      if col not in binary_cols]\n",
    "    for col in int_cols_non_binary:\n",
    "        unique_vals = data[col].nunique()\n",
    "        if unique_vals < 7:\n",
    "            data[col] = data[col].astype('category')\n",
    "        else:\n",
    "            # Discretize into 4 equal-sized groups (adjust as needed)\n",
    "            data[col] = pd.cut(data[col], bins=4, labels=False, duplicates='drop') #pd.cut does work for quantiles for us\n",
    "            # Duplicates handling: The duplicates='drop' argument means that if the bin edges are not unique (which can happen if your data is very sparse or has repeated values), duplicate bin edges will be dropped, resulting in fewer bins than specified if necessary\n",
    "            # Result: Each value in col is replaced by the bin number (as an integer) corresponding to its interval, or possibly fewer bins than 4 if duplicates are dropped.\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "64bcb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset has do be target labels and features together\n",
    "\n",
    "# Load your training and test datasets\n",
    "# train_data = pd.read_csv(\"data/train.csv\")\n",
    "# test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_data = move_targets_to_front_and_rename(data=train_data, target_label='y')\n",
    "# train_data =make_data_binary(data=train_data)\n",
    "\n",
    "# print(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee143e",
   "metadata": {},
   "source": [
    "features sollten am besten konsistent von 1 an nummeriert sein, das macht die spätere Zuordnung einfacher. Im Preprocessing wird das sowieso gemacht (Variable P im Code). So wäre also am Ende die Zuordnung um 1 verschoben.\n",
    "\n",
    "Also eigentlich ist es kein Problem wenn man das weiß. Man muss nur wissen, dass das PCT preprocessing 1,2,3,... nummeriert. Dies korrespondiert eh dann zum Datensatz an dem in Spalte 0 die target labels stehen. Da sind die features dann auch Spalte 1,2,3,...\n",
    "\n",
    "Ist aber eigentlich auch egal, weil one hot encoding eh die struktur zerstört"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6480f4",
   "metadata": {},
   "source": [
    "# Ausprobieren und zusätzliche Prozessierung von \"load_breast_cancer\" dataset\n",
    "unsinnig für nur one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "826b8e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "..  ..\n",
      "564  0\n",
      "565  0\n",
      "566  0\n",
      "567  0\n",
      "568  1\n",
      "\n",
      "[569 rows x 1 columns]\n",
      "         1      2       3       4        5        6        7        8       9  \\\n",
      "0    17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710  0.2419   \n",
      "1    20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017  0.1812   \n",
      "2    19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790  0.2069   \n",
      "3    11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520  0.2597   \n",
      "4    20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430  0.1809   \n",
      "..     ...    ...     ...     ...      ...      ...      ...      ...     ...   \n",
      "564  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890  0.1726   \n",
      "565  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791  0.1752   \n",
      "566  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302  0.1590   \n",
      "567  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200  0.2397   \n",
      "568   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000  0.1587   \n",
      "\n",
      "          10  ...     22      23      24       25       26      27      28  \\\n",
      "0    0.07871  ...  17.33  184.60  2019.0  0.16220  0.66560  0.7119  0.2654   \n",
      "1    0.05667  ...  23.41  158.80  1956.0  0.12380  0.18660  0.2416  0.1860   \n",
      "2    0.05999  ...  25.53  152.50  1709.0  0.14440  0.42450  0.4504  0.2430   \n",
      "3    0.09744  ...  26.50   98.87   567.7  0.20980  0.86630  0.6869  0.2575   \n",
      "4    0.05883  ...  16.67  152.20  1575.0  0.13740  0.20500  0.4000  0.1625   \n",
      "..       ...  ...    ...     ...     ...      ...      ...     ...     ...   \n",
      "564  0.05623  ...  26.40  166.10  2027.0  0.14100  0.21130  0.4107  0.2216   \n",
      "565  0.05533  ...  38.25  155.00  1731.0  0.11660  0.19220  0.3215  0.1628   \n",
      "566  0.05648  ...  34.12  126.70  1124.0  0.11390  0.30940  0.3403  0.1418   \n",
      "567  0.07016  ...  39.42  184.60  1821.0  0.16500  0.86810  0.9387  0.2650   \n",
      "568  0.05884  ...  30.37   59.16   268.6  0.08996  0.06444  0.0000  0.0000   \n",
      "\n",
      "         29       30  y  \n",
      "0    0.4601  0.11890  0  \n",
      "1    0.2750  0.08902  0  \n",
      "2    0.3613  0.08758  0  \n",
      "3    0.6638  0.17300  0  \n",
      "4    0.2364  0.07678  0  \n",
      "..      ...      ... ..  \n",
      "564  0.2060  0.07115  0  \n",
      "565  0.2572  0.06637  0  \n",
      "566  0.2218  0.07820  0  \n",
      "567  0.4087  0.12400  0  \n",
      "568  0.2871  0.07039  1  \n",
      "\n",
      "[569 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = pd.DataFrame(data.data), pd.DataFrame(data.target)\n",
    "y.rename(columns={0: 'y'}, inplace=True) #rename targets to 'y'\n",
    "\n",
    "print(y)\n",
    "#print(y['y'].unique())\n",
    "\n",
    "X.columns += 1 #works only if column Names are \n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "6c893d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y      1      2       3       4        5        6        7        8  \\\n",
      "0    0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
      "1    0  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
      "2    0  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
      "3    0  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
      "4    0  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
      "..  ..    ...    ...     ...     ...      ...      ...      ...      ...   \n",
      "564  0  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890   \n",
      "565  0  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791   \n",
      "566  0  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302   \n",
      "567  0  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200   \n",
      "568  1   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000   \n",
      "\n",
      "          9  ...      21     22      23      24       25       26      27  \\\n",
      "0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
      "1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
      "2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
      "3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
      "4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
      "..      ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
      "564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
      "565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
      "566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
      "567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
      "568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
      "\n",
      "         28      29       30  \n",
      "0    0.2654  0.4601  0.11890  \n",
      "1    0.1860  0.2750  0.08902  \n",
      "2    0.2430  0.3613  0.08758  \n",
      "3    0.2575  0.6638  0.17300  \n",
      "4    0.1625  0.2364  0.07678  \n",
      "..      ...     ...      ...  \n",
      "564  0.2216  0.2060  0.07115  \n",
      "565  0.1628  0.2572  0.06637  \n",
      "566  0.1418  0.2218  0.07820  \n",
      "567  0.2650  0.4087  0.12400  \n",
      "568  0.0000  0.2871  0.07039  \n",
      "\n",
      "[569 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "data = move_targets_to_front_and_rename(data=data, target_label='y')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "274b5641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y  1  2  3  4  5  6  7  8  9  ...  15331  15332  15333  15334  15335  \\\n",
      "0    1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "1    1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "2    1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "3    1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "4    1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "..  .. .. .. .. .. .. .. .. .. ..  ...    ...    ...    ...    ...    ...   \n",
      "564  1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "565  1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "566  1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "567  1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "568  2  0  0  0  1  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "\n",
      "     15336  15337  15338  15339  15340  \n",
      "0        0      0      0      0      0  \n",
      "1        0      0      0      0      0  \n",
      "2        0      0      0      0      0  \n",
      "3        0      0      0      1      0  \n",
      "4        0      0      0      0      0  \n",
      "..     ...    ...    ...    ...    ...  \n",
      "564      0      0      0      0      0  \n",
      "565      0      0      0      0      0  \n",
      "566      0      0      0      0      0  \n",
      "567      0      0      0      0      0  \n",
      "568      0      0      0      0      0  \n",
      "\n",
      "[569 rows x 15341 columns]\n"
     ]
    }
   ],
   "source": [
    "data = make_data_binary(data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5882bce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y  1  2  3  4  5  6  7  8  9  ...  15331  15332  15333  15334  15335  \\\n",
      "1    1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "8    1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "13   1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "14   1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "20   2  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "..  .. .. .. .. .. .. .. .. .. ..  ...    ...    ...    ...    ...    ...   \n",
      "552  2  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "554  2  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "560  2  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "563  1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "566  1  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "\n",
      "     15336  15337  15338  15339  15340  \n",
      "1        0      0      0      0      0  \n",
      "8        0      0      0      0      0  \n",
      "13       0      0      0      0      0  \n",
      "14       1      0      0      0      0  \n",
      "20       0      0      0      0      0  \n",
      "..     ...    ...    ...    ...    ...  \n",
      "552      0      0      0      0      0  \n",
      "554      0      0      0      0      0  \n",
      "560      0      0      0      0      0  \n",
      "563      0      0      0      0      0  \n",
      "566      0      0      0      0      0  \n",
      "\n",
      "[114 rows x 15341 columns]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Suppose your DataFrame is called 'df'\n",
    "# train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "#print(train_data) #455 rows\n",
    "print(test_data) #114 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "81865b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0             1       2             3   4                   5   \\\n",
      "0      25       Private  226802          11th   7       Never-married   \n",
      "1      38       Private   89814       HS-grad   9  Married-civ-spouse   \n",
      "2      28     Local-gov  336951    Assoc-acdm  12  Married-civ-spouse   \n",
      "3      44       Private  160323  Some-college  10  Married-civ-spouse   \n",
      "4      18             ?  103497  Some-college  10       Never-married   \n",
      "...    ..           ...     ...           ...  ..                 ...   \n",
      "16276  39       Private  215419     Bachelors  13            Divorced   \n",
      "16277  64             ?  321403       HS-grad   9             Widowed   \n",
      "16278  38       Private  374983     Bachelors  13  Married-civ-spouse   \n",
      "16279  44       Private   83891     Bachelors  13            Divorced   \n",
      "16280  35  Self-emp-inc  182148     Bachelors  13  Married-civ-spouse   \n",
      "\n",
      "                      6               7                   8       9     10  \\\n",
      "0      Machine-op-inspct       Own-child               Black    Male     0   \n",
      "1        Farming-fishing         Husband               White    Male     0   \n",
      "2        Protective-serv         Husband               White    Male     0   \n",
      "3      Machine-op-inspct         Husband               Black    Male  7688   \n",
      "4                      ?       Own-child               White  Female     0   \n",
      "...                  ...             ...                 ...     ...   ...   \n",
      "16276     Prof-specialty   Not-in-family               White  Female     0   \n",
      "16277                  ?  Other-relative               Black    Male     0   \n",
      "16278     Prof-specialty         Husband               White    Male     0   \n",
      "16279       Adm-clerical       Own-child  Asian-Pac-Islander    Male  5455   \n",
      "16280    Exec-managerial         Husband               White    Male     0   \n",
      "\n",
      "       11  12             13      14  \n",
      "0       0  40  United-States  <=50K.  \n",
      "1       0  50  United-States  <=50K.  \n",
      "2       0  40  United-States   >50K.  \n",
      "3       0  40  United-States   >50K.  \n",
      "4       0  30  United-States  <=50K.  \n",
      "...    ..  ..            ...     ...  \n",
      "16276   0  36  United-States  <=50K.  \n",
      "16277   0  40  United-States  <=50K.  \n",
      "16278   0  50  United-States  <=50K.  \n",
      "16279   0  40  United-States  <=50K.  \n",
      "16280   0  60  United-States   >50K.  \n",
      "\n",
      "[16281 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# dataset has do be target labels and features together\n",
    "\n",
    "# Load your training and test datasets\n",
    "train_data = pd.read_csv(\"data/adult/adult.data\", sep=',', skipinitialspace=True, header=None)\n",
    "test_data = pd.read_csv(\"data/adult/adult.test\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "74b8ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0             1   2             3   4                   5   \\\n",
      "0       0       Private   0          11th   1       Never-married   \n",
      "1       1       Private   0       HS-grad   2  Married-civ-spouse   \n",
      "2       0     Local-gov   0    Assoc-acdm   2  Married-civ-spouse   \n",
      "3       1       Private   0  Some-college   2  Married-civ-spouse   \n",
      "4       0             ?   0  Some-college   2       Never-married   \n",
      "...    ..           ...  ..           ...  ..                 ...   \n",
      "16276   1       Private   0     Bachelors   3            Divorced   \n",
      "16277   2             ?   0       HS-grad   2             Widowed   \n",
      "16278   1       Private   0     Bachelors   3  Married-civ-spouse   \n",
      "16279   1       Private   0     Bachelors   3            Divorced   \n",
      "16280   0  Self-emp-inc   0     Bachelors   3  Married-civ-spouse   \n",
      "\n",
      "                      6               7                   8       9   10  11  \\\n",
      "0      Machine-op-inspct       Own-child               Black    Male   0   0   \n",
      "1        Farming-fishing         Husband               White    Male   0   0   \n",
      "2        Protective-serv         Husband               White    Male   0   0   \n",
      "3      Machine-op-inspct         Husband               Black    Male   0   0   \n",
      "4                      ?       Own-child               White  Female   0   0   \n",
      "...                  ...             ...                 ...     ...  ..  ..   \n",
      "16276     Prof-specialty   Not-in-family               White  Female   0   0   \n",
      "16277                  ?  Other-relative               Black    Male   0   0   \n",
      "16278     Prof-specialty         Husband               White    Male   0   0   \n",
      "16279       Adm-clerical       Own-child  Asian-Pac-Islander    Male   0   0   \n",
      "16280    Exec-managerial         Husband               White    Male   0   0   \n",
      "\n",
      "       12             13      14  \n",
      "0       1  United-States  <=50K.  \n",
      "1       1  United-States  <=50K.  \n",
      "2       1  United-States   >50K.  \n",
      "3       1  United-States   >50K.  \n",
      "4       1  United-States  <=50K.  \n",
      "...    ..            ...     ...  \n",
      "16276   1  United-States  <=50K.  \n",
      "16277   1  United-States  <=50K.  \n",
      "16278   1  United-States  <=50K.  \n",
      "16279   1  United-States  <=50K.  \n",
      "16280   2  United-States   >50K.  \n",
      "\n",
      "[16281 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data = preprocess_numerical(test_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e0bce92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            y  0             1  2             3  4                   5  \\\n",
      "0      <=50K.  0       Private  0          11th  1       Never-married   \n",
      "1      <=50K.  1       Private  0       HS-grad  2  Married-civ-spouse   \n",
      "2       >50K.  0     Local-gov  0    Assoc-acdm  2  Married-civ-spouse   \n",
      "3       >50K.  1       Private  0  Some-college  2  Married-civ-spouse   \n",
      "4      <=50K.  0             ?  0  Some-college  2       Never-married   \n",
      "...       ... ..           ... ..           ... ..                 ...   \n",
      "16276  <=50K.  1       Private  0     Bachelors  3            Divorced   \n",
      "16277  <=50K.  2             ?  0       HS-grad  2             Widowed   \n",
      "16278  <=50K.  1       Private  0     Bachelors  3  Married-civ-spouse   \n",
      "16279  <=50K.  1       Private  0     Bachelors  3            Divorced   \n",
      "16280   >50K.  0  Self-emp-inc  0     Bachelors  3  Married-civ-spouse   \n",
      "\n",
      "                       6               7                   8       9  10  11  \\\n",
      "0      Machine-op-inspct       Own-child               Black    Male   0   0   \n",
      "1        Farming-fishing         Husband               White    Male   0   0   \n",
      "2        Protective-serv         Husband               White    Male   0   0   \n",
      "3      Machine-op-inspct         Husband               Black    Male   0   0   \n",
      "4                      ?       Own-child               White  Female   0   0   \n",
      "...                  ...             ...                 ...     ...  ..  ..   \n",
      "16276     Prof-specialty   Not-in-family               White  Female   0   0   \n",
      "16277                  ?  Other-relative               Black    Male   0   0   \n",
      "16278     Prof-specialty         Husband               White    Male   0   0   \n",
      "16279       Adm-clerical       Own-child  Asian-Pac-Islander    Male   0   0   \n",
      "16280    Exec-managerial         Husband               White    Male   0   0   \n",
      "\n",
      "       12             13  \n",
      "0       1  United-States  \n",
      "1       1  United-States  \n",
      "2       1  United-States  \n",
      "3       1  United-States  \n",
      "4       1  United-States  \n",
      "...    ..            ...  \n",
      "16276   1  United-States  \n",
      "16277   1  United-States  \n",
      "16278   1  United-States  \n",
      "16279   1  United-States  \n",
      "16280   2  United-States  \n",
      "\n",
      "[16281 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data = move_targets_to_front_and_rename(data= test_data , target_label=14)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "518b4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y  1  2  3  4  5  6  7  8  9  ...  114  115  116  117  118  119  120  \\\n",
      "0      1  1  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2      2  1  1  0  0  0  0  0  1  0  ...    0    0    0    0    0    0    0   \n",
      "3      2  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4      1  0  1  0  0  0  1  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "...   .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "16276  1  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16277  1  1  0  0  1  0  1  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16278  1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16279  1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16280  2  1  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "       121  122  123  \n",
      "0        1    0    0  \n",
      "1        1    0    0  \n",
      "2        1    0    0  \n",
      "3        1    0    0  \n",
      "4        1    0    0  \n",
      "...    ...  ...  ...  \n",
      "16276    1    0    0  \n",
      "16277    1    0    0  \n",
      "16278    1    0    0  \n",
      "16279    1    0    0  \n",
      "16280    1    0    0  \n",
      "\n",
      "[16281 rows x 124 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data = make_data_binary(test_data) # dass hier nur 124 Spalten rauskommen bedeutet, dass die Testdaten anders bootstrapped/preprocessed werden, als die Traningsdaten\n",
    "print(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "54d94a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y  1  2  3  4  5  6  7  8  9  ...  115  116  117  118  119  120  121  \\\n",
      "0      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "3      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4      1  0  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "...   .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "32556  1  0  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "32557  2  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "32558  1  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "32559  1  1  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "32560  2  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "       122  123  124  \n",
      "0        1    0    0  \n",
      "1        1    0    0  \n",
      "2        1    0    0  \n",
      "3        1    0    0  \n",
      "4        0    0    0  \n",
      "...    ...  ...  ...  \n",
      "32556    1    0    0  \n",
      "32557    1    0    0  \n",
      "32558    1    0    0  \n",
      "32559    1    0    0  \n",
      "32560    1    0    0  \n",
      "\n",
      "[32561 rows x 125 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = preprocess_numerical(train_data)\n",
    "train_data = move_targets_to_front_and_rename(data= train_data , target_label=14)\n",
    "train_data = make_data_binary(train_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4e92ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<=50K' '>50K']\n",
      "       y  1  2  3  4  5  6  7  8  9  ...  115  116  117  118  119  120  121  \\\n",
      "0      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "3      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4      1  0  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "...   .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "16276  1  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16277  1  1  0  0  1  0  1  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16278  1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16279  1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16280  2  1  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "       122  123  124  \n",
      "0        1    0    0  \n",
      "1        1    0    0  \n",
      "2        1    0    0  \n",
      "3        1    0    0  \n",
      "4        0    0    0  \n",
      "...    ...  ...  ...  \n",
      "16276    1    0    0  \n",
      "16277    1    0    0  \n",
      "16278    1    0    0  \n",
      "16279    1    0    0  \n",
      "16280    1    0    0  \n",
      "\n",
      "[48842 rows x 125 columns]\n"
     ]
    }
   ],
   "source": [
    "# wenn man adult dataset training und testdaten unabhängig voneinander preprozessiert, dann sorgt das dafür, dass in den trainingsdaten eine spalte da ist,\n",
    "# welche beim preprozessieren der testdaten nicht auftaucht\n",
    "# => beide datensätze zusammenfügen, preprozessieren und dann die letzten 16281 rows (testdaten) wieder aus dem datensatz holen\n",
    "# das sollte auch nichts ändern, da es als binär categorical erkannt wird, also \n",
    "\n",
    "# dataset has do be target labels and features together\n",
    "\n",
    "# Load your training and test datasets\n",
    "train_data = pd.read_csv(\"data/adult/adult.data\", sep=',', skipinitialspace=True, header=None) #32561 rows\n",
    "test_data = pd.read_csv(\"data/adult/adult.test\", sep=',', skipinitialspace=True, header=None) #16281 rows\n",
    "\n",
    "# Remove dots from the 'target' column\n",
    "test_data[14] = test_data[14].astype(str).str.replace('.', '', regex=False)\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "#print(len_train_data)\n",
    "#len_test_data = len(test_data)\n",
    "\n",
    "\n",
    "stacked = pd.concat([train_data, test_data ], ignore_index=False)\n",
    "#print(stacked)\n",
    "\n",
    "\n",
    "stacked = preprocess_numerical(stacked)\n",
    "stacked = move_targets_to_front_and_rename(data= stacked, target_label=14)\n",
    "\n",
    "unique_values = stacked ['y'].unique()\n",
    "print(unique_values) # hier sieht man dann, dass es einen zusätzlichen punkt bei den target labels in adult.test gibt; also ohne löschen des punktes\n",
    "\n",
    "\n",
    "\n",
    "stacked = make_data_binary(stacked)\n",
    "print(stacked)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_bin = stacked.iloc[:len_train_data] #[32561 rows x 125 columns]\n",
    "test_data_bin = stacked.iloc[len_train_data:] #[16281 rows x 125 columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c4693d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y  1  2  3  4  5  6  7  8  9  ...  115  116  117  118  119  120  121  \\\n",
      "0      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "3      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4      1  0  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "...   .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "32556  1  0  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "32557  2  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "32558  1  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "32559  1  1  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "32560  2  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "       122  123  124  \n",
      "0        1    0    0  \n",
      "1        1    0    0  \n",
      "2        1    0    0  \n",
      "3        1    0    0  \n",
      "4        0    0    0  \n",
      "...    ...  ...  ...  \n",
      "32556    1    0    0  \n",
      "32557    1    0    0  \n",
      "32558    1    0    0  \n",
      "32559    1    0    0  \n",
      "32560    1    0    0  \n",
      "\n",
      "[32561 rows x 125 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "8af2ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y  1  2  3  4  5  6  7  8  9  ...  115  116  117  118  119  120  121  \\\n",
      "0      1  1  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1      1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2      2  1  1  0  0  0  0  0  1  0  ...    0    0    0    0    0    0    0   \n",
      "3      2  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4      1  0  1  0  0  0  1  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "...   .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "16276  1  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16277  1  1  0  0  1  0  1  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16278  1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16279  1  1  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16280  2  1  1  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "       122  123  124  \n",
      "0        1    0    0  \n",
      "1        1    0    0  \n",
      "2        1    0    0  \n",
      "3        1    0    0  \n",
      "4        1    0    0  \n",
      "...    ...  ...  ...  \n",
      "16276    1    0    0  \n",
      "16277    1    0    0  \n",
      "16278    1    0    0  \n",
      "16279    1    0    0  \n",
      "16280    1    0    0  \n",
      "\n",
      "[16281 rows x 125 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_data_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "951f70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data_bin\n",
    "test_data=test_data_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8e9f775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = \"y\"\n",
    "depth_rolling_tree = 3\n",
    "criterion_loss = \"gini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "8887605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n",
      "       ...\n",
      "       115, 116, 117, 118, 119, 120, 121, 122, 123, 124],\n",
      "      dtype='object', length=124)\n"
     ]
    }
   ],
   "source": [
    "# get features\n",
    "feature_columns = train_data.columns[1:] #assuming labels are in first column, ensured trough move_targets_to_front_and_rename()\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348a545",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (600210870.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[310]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# solving with pulp\n",
    "\n",
    "start_time_pulp = time.time()\n",
    "\n",
    "# Run the classifier using pulp\n",
    "result_dict_pulp, result_df_test_data_pulp, result_df_training_data_pulp = rollo_oct_pulp.run(\n",
    "                                                                train=train_data,\n",
    "                                                                test=test_data,\n",
    "                                                                target_label=\"y\",\n",
    "                                                                features=feature_columns,\n",
    "                                                                depth=depth_rolling_tree,\n",
    "                                                                criterion=criterion_loss\n",
    ")\n",
    "end_time_pulp = time.time()\n",
    "print(f\"Pulp execution time for depth {depth_rolling_tree} : {end_time_pulp - start_time_pulp} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: {'training_accuracy': 0.8215349651423483, 'test_accuracy': 0.0, 'time': 172.7234263420105}, 2: {'training_accuracy': 0.8193544424311293, 'test_accuracy': 0.0, 'time': 342.99163269996643}}\n"
     ]
    }
   ],
   "source": [
    "print(result_dict_pulp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119afaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y  prediction  leaf\n",
      "0      2           1    13\n",
      "1      2           1     9\n",
      "2      4           1     8\n",
      "3      4           1     9\n",
      "4      2           1    13\n",
      "...   ..         ...   ...\n",
      "16276  2           1    12\n",
      "16277  2           1    13\n",
      "16278  2           3    10\n",
      "16279  2           1    12\n",
      "16280  4           3    10\n",
      "\n",
      "[16281 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(result_df_test_data_pulp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a63ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y  prediction  leaf\n",
      "0      1           1    12\n",
      "1      1           3    10\n",
      "2      1           1    13\n",
      "3      1           1    11\n",
      "4      1           3    10\n",
      "...   ..         ...   ...\n",
      "32556  1           1     8\n",
      "32557  3           1     9\n",
      "32558  1           1    13\n",
      "32559  1           1    13\n",
      "32560  3           1     9\n",
      "\n",
      "[32561 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(result_df_training_data_pulp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
