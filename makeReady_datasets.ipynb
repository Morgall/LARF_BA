{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0e8851",
   "metadata": {},
   "source": [
    "Hier alle vorbereitungen für x-fold cross validation inkl. dem anschließenden spreichern der vorbereiteten binarized datensätzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d97db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from helpers.helpers import preprocess_numerical, move_targets_to_front_and_rename, make_data_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d454ef72",
   "metadata": {},
   "source": [
    "# test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ce0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"datasets/example_datasets/train.csv\")\n",
    "test_data = pd.read_csv(\"datasets/example_datasets/test.csv\")\n",
    "\n",
    "stacked = pd.concat([train_data, test_data], ignore_index=False)\n",
    "\n",
    "#stacked.to_csv('datasets/example_datasets/stacked.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a97bcf0",
   "metadata": {},
   "source": [
    "# adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1484d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<=50K' '>50K']\n",
      "       y  1  2  3  4  5  6  7  8  9  ...  112  113  114  115  116  117  118  \\\n",
      "0      1  1  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "1      1  1  0  0  0  1  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "2      1  1  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "3      1  1  0  0  0  0  1  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "4      1  0  0  1  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "...   .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "16276  1  0  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16277  1  1  0  0  0  0  1  1  0  0  ...    0    0    0    0    0    0    0   \n",
      "16278  1  1  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16279  1  1  0  0  0  1  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "16280  2  1  0  0  1  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
      "\n",
      "       119  120  121  \n",
      "0        1    0    0  \n",
      "1        1    0    0  \n",
      "2        1    0    0  \n",
      "3        1    0    0  \n",
      "4        0    0    0  \n",
      "...    ...  ...  ...  \n",
      "16276    1    0    0  \n",
      "16277    1    0    0  \n",
      "16278    1    0    0  \n",
      "16279    1    0    0  \n",
      "16280    1    0    0  \n",
      "\n",
      "[48842 rows x 122 columns]\n",
      "  original  transformed\n",
      "0    <=50K            1\n",
      "7     >50K            2\n"
     ]
    }
   ],
   "source": [
    "# Load your training and test datasets\n",
    "train_data = pd.read_csv(\"datasets/adult/adult.data\", sep=',', skipinitialspace=True, header=None) #32561 rows\n",
    "test_data = pd.read_csv(\"datasets/adult/adult.test\", sep=',', skipinitialspace=True, header=None) #16281 rows\n",
    "\n",
    "\n",
    "# Remove dots from the 'target' column\n",
    "test_data[14] = test_data[14].astype(str).str.replace('.', '', regex=False)\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "\n",
    "stacked = pd.concat([train_data, test_data ], ignore_index=False)\n",
    "#print(stacked)\n",
    "\n",
    "\n",
    "stacked = preprocess_numerical(stacked, target_label=14)\n",
    "stacked = move_targets_to_front_and_rename(data= stacked, target_label=14)\n",
    "\n",
    "unique_values = stacked['y'].unique()\n",
    "print(unique_values) # hier sieht man dann, dass es einen zusätzlichen punkt bei den target labels in adult.test gibt; also ohne löschen des punktes\n",
    "\n",
    "target_vals_col_orig = stacked['y']\n",
    "\n",
    "\n",
    "stacked = make_data_binary(stacked)\n",
    "print(stacked)\n",
    "\n",
    "#stacked.to_csv('datasets/adult/stacked.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = stacked['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/adult/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8381195",
   "metadata": {},
   "source": [
    "# car eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b5dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unacc' 'acc' 'vgood' 'good']\n",
      "      y  1  2  3  4  5  6  7  8  9  ...  12  13  14  15  16  17  18  19  20  \\\n",
      "0     3  0  0  0  1  0  0  0  1  1  ...   0   1   0   0   0   0   1   0   1   \n",
      "1     3  0  0  0  1  0  0  0  1  1  ...   0   1   0   0   0   0   1   0   0   \n",
      "2     3  0  0  0  1  0  0  0  1  1  ...   0   1   0   0   0   0   1   1   0   \n",
      "3     3  0  0  0  1  0  0  0  1  1  ...   0   1   0   0   0   1   0   0   1   \n",
      "4     3  0  0  0  1  0  0  0  1  1  ...   0   1   0   0   0   1   0   0   0   \n",
      "...  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "1723  2  0  1  0  0  0  1  0  0  0  ...   1   0   0   1   0   1   0   0   0   \n",
      "1724  4  0  1  0  0  0  1  0  0  0  ...   1   0   0   1   0   1   0   1   0   \n",
      "1725  3  0  1  0  0  0  1  0  0  0  ...   1   0   0   1   1   0   0   0   1   \n",
      "1726  2  0  1  0  0  0  1  0  0  0  ...   1   0   0   1   1   0   0   0   0   \n",
      "1727  4  0  1  0  0  0  1  0  0  0  ...   1   0   0   1   1   0   0   1   0   \n",
      "\n",
      "      21  \n",
      "0      0  \n",
      "1      1  \n",
      "2      0  \n",
      "3      0  \n",
      "4      1  \n",
      "...   ..  \n",
      "1723   1  \n",
      "1724   0  \n",
      "1725   0  \n",
      "1726   1  \n",
      "1727   0  \n",
      "\n",
      "[1728 rows x 22 columns]\n",
      "     original  transformed\n",
      "0       unacc            3\n",
      "227       acc            1\n",
      "1097    vgood            4\n",
      "1199     good            2\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/car_evaluation/car.data\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=6)\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=6)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "#data.to_csv('datasets/car_evaluation/car_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/car_evaluation/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f65c7",
   "metadata": {},
   "source": [
    "# seismic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8118d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "      y  1  2  3  4  5  6  7  8  9  ...  38  39  40  41  42  43  44  45  46  \\\n",
      "0     1  0  0  0  0  0  1  0  0  0  ...   0   1   1   1   1   0   0   1   0   \n",
      "1     1  0  0  0  0  0  1  0  0  0  ...   0   1   1   1   0   1   0   0   1   \n",
      "2     1  0  0  0  0  0  1  0  0  1  ...   0   1   1   1   1   0   0   1   0   \n",
      "3     1  0  0  0  0  0  1  0  0  0  ...   0   1   1   1   0   1   0   0   1   \n",
      "4     1  0  0  0  0  0  1  0  0  0  ...   0   1   1   1   1   0   0   1   0   \n",
      "...  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "2579  1  1  1  0  0  0  1  0  0  0  ...   0   1   1   1   1   0   0   1   0   \n",
      "2580  1  1  1  0  0  0  1  0  0  0  ...   0   1   1   1   1   0   0   1   0   \n",
      "2581  1  1  1  0  0  0  1  0  0  0  ...   0   1   1   1   1   0   0   1   0   \n",
      "2582  1  0  1  0  0  0  1  0  0  0  ...   0   1   1   1   1   0   0   1   0   \n",
      "2583  1  0  1  0  0  0  1  0  0  0  ...   0   1   1   1   1   0   0   1   0   \n",
      "\n",
      "      47  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "...   ..  \n",
      "2579   0  \n",
      "2580   0  \n",
      "2581   0  \n",
      "2582   0  \n",
      "2583   0  \n",
      "\n",
      "[2584 rows x 48 columns]\n",
      "    original  transformed\n",
      "0          0            1\n",
      "35         1            2\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/seismic/seismic_data.txt\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=18)\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=18)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "#data.to_csv('datasets/seismic/seismic_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/seismic/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39df9c",
   "metadata": {},
   "source": [
    "# wine\n",
    "\n",
    "Hier sieht man Problem, wenn cass identifier integers und nicht binary sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd657611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y  1  2  3  4  5  6  7  8  9  10  11  12  13\n",
      "0    1  4  1  3  0  4  3  4  1  4   3   2   4   4\n",
      "1    1  2  2  0  0  2  3  3  0  1   2   3   4   4\n",
      "2    1  2  3  4  2  2  3  4  1  4   3   2   3   4\n",
      "3    1  4  2  3  0  4  4  4  0  4   4   1   4   4\n",
      "4    1  2  3  4  3  4  3  3  2  3   2   2   3   2\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ..  ..  ..  ..\n",
      "173  3  3  4  3  3  2  1  0  4  0   4   0   0   2\n",
      "174  3  3  4  3  4  3  1  0  3  1   4   0   0   3\n",
      "175  3  2  4  1  2  4  0  0  3  1   4   0   0   3\n",
      "176  3  2  3  2  2  4  0  0  4  2   4   0   0   3\n",
      "177  3  4  4  4  4  2  1  0  4  1   4   0   0   1\n",
      "\n",
      "[178 rows x 14 columns]\n",
      "[1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhier hat man gesehen, dass target vars auch categorical gemacht wurden, was zu problemen geführt hat.#\\nDas wurde mit der exclusion von der target label col in preprocess_numerical() behoben\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/wine/wine.data\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=0)\n",
    "\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=0)\n",
    "\n",
    "print(data)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "\"\"\"\n",
    "hier hat man gesehen, dass target vars auch categorical gemacht wurden, was zu problemen geführt hat.#\n",
    "Das wurde mit der exclusion von der target label col in preprocess_numerical() behoben\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "942ece55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y  1  2  3  4  5  6  7  8  9  ...  56  57  58  59  60  61  62  63  64  65\n",
      "0    1  0  0  0  0  1  0  1  0  0  ...   0   0   0   0   1   0   0   0   0   1\n",
      "1    1  0  0  1  0  0  0  0  1  0  ...   0   0   0   0   1   0   0   0   0   1\n",
      "2    1  0  0  1  0  0  0  0  0  1  ...   0   0   0   1   0   0   0   0   0   1\n",
      "3    1  0  0  0  0  1  0  0  1  0  ...   0   0   0   0   1   0   0   0   0   1\n",
      "4    1  0  0  1  0  0  0  0  0  1  ...   0   0   0   1   0   0   0   1   0   0\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
      "173  3  0  0  0  1  0  0  0  0  0  ...   1   0   0   0   0   0   0   1   0   0\n",
      "174  3  0  0  0  1  0  0  0  0  0  ...   1   0   0   0   0   0   0   0   1   0\n",
      "175  3  0  0  1  0  0  0  0  0  0  ...   1   0   0   0   0   0   0   0   1   0\n",
      "176  3  0  0  1  0  0  0  0  0  1  ...   1   0   0   0   0   0   0   0   1   0\n",
      "177  3  0  0  0  0  1  0  0  0  0  ...   1   0   0   0   0   0   1   0   0   0\n",
      "\n",
      "[178 rows x 66 columns]\n",
      "     original  transformed\n",
      "0           1            1\n",
      "59          2            2\n",
      "130         3            3\n"
     ]
    }
   ],
   "source": [
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "\n",
    "#data.to_csv('datasets/wine/wine_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/wine/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9e914",
   "metadata": {},
   "source": [
    "# spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "983719a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4601\n",
      "[1 0]\n",
      "      y  1  2  3  4  5  6  7  8  9  ...  73  74  75  76  77  78  79  80  81  \\\n",
      "0     2  0  0  0  0  1  0  1  0  0  ...   0   0   0   0   1   0   0   0   1   \n",
      "1     2  1  0  1  1  0  0  1  0  0  ...   0   0   0   0   1   0   0   0   0   \n",
      "2     2  0  1  1  0  0  0  1  0  0  ...   0   0   0   0   1   0   0   0   0   \n",
      "3     2  0  1  0  1  1  0  0  0  0  ...   0   0   0   1   0   0   0   0   1   \n",
      "4     2  0  1  0  1  1  0  0  0  0  ...   0   0   0   1   0   0   0   0   1   \n",
      "...  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "4596  1  1  0  1  0  0  0  0  0  0  ...   1   0   0   0   0   0   0   1   0   \n",
      "4597  1  0  0  0  0  0  0  0  0  0  ...   1   0   0   0   0   1   0   0   0   \n",
      "4598  1  1  0  0  0  0  0  1  0  0  ...   0   1   0   0   0   0   0   1   0   \n",
      "4599  1  1  0  0  0  0  0  0  0  0  ...   1   0   0   0   0   0   0   1   0   \n",
      "4600  1  0  0  0  0  0  0  0  0  1  ...   1   0   0   0   0   0   1   0   0   \n",
      "\n",
      "      82  \n",
      "0      0  \n",
      "1      1  \n",
      "2      1  \n",
      "3      0  \n",
      "4      0  \n",
      "...   ..  \n",
      "4596   0  \n",
      "4597   0  \n",
      "4598   0  \n",
      "4599   0  \n",
      "4600   0  \n",
      "\n",
      "[4601 rows x 83 columns]\n",
      "      original  transformed\n",
      "0            1            2\n",
      "1813         0            1\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/spambase/spambase.data\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "#num_columns = data.shape[1]\n",
    "#print(num_columns)\n",
    "\n",
    "num_rows = data.shape[0]\n",
    "print(num_rows)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=57)\n",
    "\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=57)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "\n",
    "#data.to_csv('datasets/spambase/spambase_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/spambase/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2712f",
   "metadata": {},
   "source": [
    "# wisconsin breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3db2469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'B']\n",
      "     y  1  2  3  4  5  6  7  8  9  ...  141  142  143  144  145  146  147  \\\n",
      "0    2  0  0  0  0  1  1  0  0  0  ...    0    0    0    0    1    0    0   \n",
      "1    2  0  0  0  0  1  0  1  0  0  ...    0    0    1    0    0    0    0   \n",
      "2    2  0  0  0  0  1  0  0  0  1  ...    0    0    0    0    1    0    0   \n",
      "3    2  0  1  0  0  0  0  0  0  1  ...    0    0    0    0    1    0    0   \n",
      "4    2  0  0  0  0  1  1  0  0  0  ...    1    0    0    0    0    0    1   \n",
      "..  .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "564  2  0  0  0  0  1  0  0  0  1  ...    1    0    0    0    0    0    1   \n",
      "565  2  0  0  0  0  1  0  0  0  0  ...    0    1    0    0    0    1    0   \n",
      "566  2  0  0  0  1  0  0  0  0  0  ...    1    0    0    0    0    0    0   \n",
      "567  2  0  0  0  0  1  0  0  0  0  ...    0    0    0    0    1    0    0   \n",
      "568  1  1  0  0  0  0  0  0  0  0  ...    0    0    1    0    0    0    1   \n",
      "\n",
      "     148  149  150  \n",
      "0      0    0    1  \n",
      "1      0    1    0  \n",
      "2      0    1    0  \n",
      "3      0    0    1  \n",
      "4      0    0    0  \n",
      "..   ...  ...  ...  \n",
      "564    0    0    0  \n",
      "565    0    0    0  \n",
      "566    1    0    0  \n",
      "567    0    0    1  \n",
      "568    0    0    0  \n",
      "\n",
      "[569 rows x 151 columns]\n",
      "   original  transformed\n",
      "0         M            2\n",
      "19        B            1\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/breast+cancer+wisconsin+diagnostic/wdbc.data\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "data.drop(data.columns[0], axis=1, inplace=True) #drop the id col; result in col 0 not existing. Should get fixed during binaring\n",
    "#print(data)\n",
    "\n",
    "#num_columns = data.shape[1]\n",
    "#print(num_columns)\n",
    "\n",
    "#num_rows = data.shape[0]\n",
    "#print(num_rows)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=1)\n",
    "\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=1)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "\n",
    "#data.to_csv('datasets/breast+cancer+wisconsin+diagnostic/wdbc_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/breast+cancer+wisconsin+diagnostic/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440799d",
   "metadata": {},
   "source": [
    "# Nursery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805e7135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "12960\n",
      "['recommend' 'priority' 'not_recom' 'very_recom' 'spec_prior']\n",
      "       y  1  2  3  4  5  6  7  8  9  ...  17  18  19  20  21  22  23  24  25  \\\n",
      "0      3  0  0  0  1  0  0  0  1  0  ...   0   1   0   0   1   0   0   0   0   \n",
      "1      2  0  0  0  1  0  0  0  1  0  ...   0   1   0   0   1   0   0   0   1   \n",
      "2      1  0  0  0  1  0  0  0  1  0  ...   0   1   0   0   1   0   0   1   0   \n",
      "3      3  0  0  0  1  0  0  0  1  0  ...   0   1   0   0   0   0   1   0   0   \n",
      "4      2  0  0  0  1  0  0  0  1  0  ...   0   1   0   0   0   0   1   0   1   \n",
      "...   .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "12955  4  1  1  0  0  0  0  0  0  1  ...   1   0   1   0   0   0   1   0   1   \n",
      "12956  1  1  1  0  0  0  0  0  0  1  ...   1   0   1   0   0   0   1   1   0   \n",
      "12957  4  1  1  0  0  0  0  0  0  1  ...   1   0   1   0   0   1   0   0   0   \n",
      "12958  4  1  1  0  0  0  0  0  0  1  ...   1   0   1   0   0   1   0   0   1   \n",
      "12959  1  1  1  0  0  0  0  0  0  1  ...   1   0   1   0   0   1   0   1   0   \n",
      "\n",
      "       26  \n",
      "0       1  \n",
      "1       0  \n",
      "2       0  \n",
      "3       1  \n",
      "4       0  \n",
      "...    ..  \n",
      "12955   0  \n",
      "12956   0  \n",
      "12957   1  \n",
      "12958   0  \n",
      "12959   0  \n",
      "\n",
      "[12960 rows x 27 columns]\n",
      "        original  transformed\n",
      "0      recommend            3\n",
      "1       priority            2\n",
      "2      not_recom            1\n",
      "9     very_recom            5\n",
      "2683  spec_prior            4\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/nursery/nursery.data\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "\n",
    "num_columns = data.shape[1]\n",
    "print(num_columns)\n",
    "\n",
    "num_rows = data.shape[0]\n",
    "print(num_rows)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=8)\n",
    "\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=8)\n",
    "\n",
    "#num_columns = data.shape[1]\n",
    "#print(num_columns)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "\n",
    "#data.to_csv('datasets/nursery/nursery_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/nursery/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74140ae1",
   "metadata": {},
   "source": [
    "# Mushroom Agaricus lepiota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d8fc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "['p' 'e']\n",
      "      y  1  2  3  4  5  6  7  8  9  ...  103  104  105  106  107  108  109  \\\n",
      "0     2  1  1  0  1  0  0  0  0  0  ...    1    0    0    0    0    0    0   \n",
      "1     1  1  1  0  0  0  0  0  0  0  ...    0    0    0    0    1    0    0   \n",
      "2     1  1  1  0  0  0  1  0  0  0  ...    0    0    0    0    0    0    1   \n",
      "3     2  1  1  0  1  0  0  0  0  0  ...    1    0    0    0    0    0    0   \n",
      "4     1  0  1  1  0  1  0  0  0  0  ...    0    0    0    0    1    0    0   \n",
      "...  .. .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "8119  1  0  0  0  0  0  0  0  0  1  ...    0    0    0    0    0    1    0   \n",
      "8120  1  0  0  0  0  0  0  0  0  0  ...    0    1    0    0    0    1    0   \n",
      "8121  1  0  0  0  0  0  0  0  1  0  ...    0    0    0    0    0    1    0   \n",
      "8122  2  0  1  0  1  1  0  0  0  1  ...    0    1    0    0    0    1    0   \n",
      "8123  1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    1    0   \n",
      "\n",
      "      110  111  112  \n",
      "0       0    1    0  \n",
      "1       0    0    0  \n",
      "2       0    0    0  \n",
      "3       0    1    0  \n",
      "4       0    0    0  \n",
      "...   ...  ...  ...  \n",
      "8119    0    0    0  \n",
      "8120    0    0    0  \n",
      "8121    0    0    0  \n",
      "8122    0    0    0  \n",
      "8123    0    0    0  \n",
      "\n",
      "[8124 rows x 113 columns]\n",
      "  original  transformed\n",
      "0        p            2\n",
      "1        e            1\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/mushroom/agaricus-lepiota.data\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "num_columns = data.shape[1]\n",
    "print(num_columns)\n",
    "\n",
    "#num_rows = data.shape[0]\n",
    "#print(num_rows)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=0)\n",
    "\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=0)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "\n",
    "#data.to_csv('datasets/mushroom/agaricus_lepiota_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/mushroom/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f678f3",
   "metadata": {},
   "source": [
    "# Banknote Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5daf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0 1]\n",
      "      y  1  2  3  4  5  6  7  8  9  ...  11  12  13  14  15  16  17  18  19  \\\n",
      "0     1  0  0  0  0  1  0  0  0  0  ...   1   0   0   0   0   0   0   1   0   \n",
      "1     1  0  0  0  0  1  0  0  0  0  ...   1   0   0   0   0   0   1   0   0   \n",
      "2     1  0  0  0  0  1  0  1  0  0  ...   0   0   0   1   0   0   0   0   1   \n",
      "3     1  0  0  0  0  1  0  0  0  0  ...   1   0   0   0   0   1   0   0   0   \n",
      "4     1  0  0  1  0  0  1  0  0  0  ...   0   0   0   0   1   0   0   1   0   \n",
      "...  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "1367  2  0  0  1  0  0  0  0  1  0  ...   0   1   0   0   0   0   0   1   0   \n",
      "1368  2  0  1  0  0  0  1  0  0  0  ...   0   0   0   0   1   0   0   0   1   \n",
      "1369  2  1  0  0  0  0  1  0  0  0  ...   0   0   0   0   1   0   1   0   0   \n",
      "1370  2  1  0  0  0  0  1  0  0  0  ...   0   0   0   0   1   0   1   0   0   \n",
      "1371  2  1  0  0  0  0  0  1  0  0  ...   0   0   0   1   0   0   0   0   0   \n",
      "\n",
      "      20  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "...   ..  \n",
      "1367   0  \n",
      "1368   0  \n",
      "1369   0  \n",
      "1370   0  \n",
      "1371   1  \n",
      "\n",
      "[1372 rows x 21 columns]\n",
      "     original  transformed\n",
      "0           0            1\n",
      "762         1            2\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/banknote+authentication/data_banknote_authentication.txt\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "num_columns = data.shape[1]\n",
    "print(num_columns)\n",
    "\n",
    "#num_rows = data.shape[0]\n",
    "#print(num_rows)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=4)\n",
    "\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=4)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "\n",
    "#data.to_csv('datasets/banknote+authentication/banknote_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/banknote+authentication/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c478d4",
   "metadata": {},
   "source": [
    "king rook vs. king pawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7795e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['won' 'nowin']\n",
      "      y  1  2  3  4  5  6  7  8  9  ...  29  30  31  32  33  34  35  36  37  \\\n",
      "0     2  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   1   1   0   0   1   \n",
      "1     2  0  0  0  0  1  0  0  0  0  ...   0   0   0   0   1   1   0   0   1   \n",
      "2     2  0  0  0  0  1  0  1  0  0  ...   0   0   0   0   1   1   0   0   1   \n",
      "3     2  0  0  0  0  0  0  0  0  1  ...   0   0   0   0   1   1   0   0   1   \n",
      "4     2  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   1   1   0   0   1   \n",
      "...  .. .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "3191  1  1  0  0  0  0  0  1  0  0  ...   0   0   1   0   1   0   0   1   0   \n",
      "3192  1  1  0  0  0  0  0  1  0  0  ...   0   0   1   0   1   0   0   1   0   \n",
      "3193  1  1  0  0  0  0  0  1  0  0  ...   0   0   1   0   1   0   0   0   1   \n",
      "3194  1  1  0  1  0  0  0  1  0  0  ...   0   0   1   0   0   0   0   1   0   \n",
      "3195  1  1  0  1  0  0  0  1  0  0  ...   0   0   1   0   0   0   0   0   1   \n",
      "\n",
      "      38  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "...   ..  \n",
      "3191   0  \n",
      "3192   0  \n",
      "3193   0  \n",
      "3194   0  \n",
      "3195   0  \n",
      "\n",
      "[3196 rows x 39 columns]\n",
      "    original  transformed\n",
      "0        won            2\n",
      "904    nowin            1\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/chess+king+rook+vs+king+pawn/kr-vs-kp.data\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "num_columns = data.shape[1]\n",
    "print(num_columns)\n",
    "\n",
    "#num_rows = data.shape[0]\n",
    "#print(num_rows)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=36)\n",
    "\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=36)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "\n",
    "#data.to_csv('datasets/chess+king+rook+vs+king+pawn/kr-vs-kp_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/chess+king+rook+vs+king+pawn/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791e99a",
   "metadata": {},
   "source": [
    "monk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5087cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1  2  3  4  5  6\n",
      "0    1  1  1  1  1  3  1\n",
      "1    1  1  1  1  1  3  2\n",
      "2    1  1  1  1  3  2  1\n",
      "3    1  1  1  1  3  3  2\n",
      "4    1  1  1  2  1  2  1\n",
      "..  .. .. .. .. .. .. ..\n",
      "427  1  3  3  2  3  2  2\n",
      "428  1  3  3  2  3  3  1\n",
      "429  1  3  3  2  3  3  2\n",
      "430  1  3  3  2  3  4  1\n",
      "431  1  3  3  2  3  4  2\n",
      "\n",
      "[556 rows x 7 columns]\n",
      "[1 0]\n",
      "     y  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15\n",
      "0    2  0  0  1  0  0  1  0  0  1   0   0   0   0   1   0\n",
      "1    2  0  1  1  0  0  1  0  0  1   0   0   0   0   1   0\n",
      "2    2  0  0  1  0  0  1  0  0  0   0   1   0   1   0   0\n",
      "3    2  0  1  1  0  0  1  0  0  0   0   1   0   0   1   0\n",
      "4    2  1  0  1  0  0  1  0  0  1   0   0   0   1   0   0\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ..  ..  ..  ..  ..  ..\n",
      "427  2  1  1  0  0  1  0  0  1  0   0   1   0   1   0   0\n",
      "428  2  1  0  0  0  1  0  0  1  0   0   1   0   0   1   0\n",
      "429  2  1  1  0  0  1  0  0  1  0   0   1   0   0   1   0\n",
      "430  2  1  0  0  0  1  0  0  1  0   0   1   0   0   0   1\n",
      "431  2  1  1  0  0  1  0  0  1  0   0   1   0   0   0   1\n",
      "\n",
      "[556 rows x 16 columns]\n",
      "    original  transformed\n",
      "0          1            2\n",
      "10         0            1\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"datasets/monk+s+problems/monks-1.train\", sep=' ', skipinitialspace=True, header=None)\n",
    "test_data = pd.read_csv(\"datasets/monk+s+problems/monks-1.test\", sep=' ', skipinitialspace=True, header=None)\n",
    "\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "\n",
    "stacked = pd.concat([train_data, test_data], ignore_index=False)\n",
    "#print(stacked)\n",
    "\n",
    "stacked = stacked.drop(columns=[7])\n",
    "\n",
    "print(stacked)\n",
    "\n",
    "\n",
    "stacked = preprocess_numerical(stacked, target_label=0)\n",
    "stacked = move_targets_to_front_and_rename(data= stacked, target_label=0)\n",
    "\n",
    "unique_values = stacked['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = stacked['y']\n",
    "\n",
    "\n",
    "stacked = make_data_binary(stacked)\n",
    "print(stacked)\n",
    "\n",
    "stacked.to_csv('datasets/monk1/monk1_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = stacked['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/monk1/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c03f0",
   "metadata": {},
   "source": [
    "monk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1544b372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1  2  3  4  5  6\n",
      "0    0  1  1  1  1  2  2\n",
      "1    0  1  1  1  1  4  1\n",
      "2    0  1  1  1  2  1  1\n",
      "3    0  1  1  1  2  1  2\n",
      "4    0  1  1  1  2  2  1\n",
      "..  .. .. .. .. .. .. ..\n",
      "427  0  3  3  2  3  2  2\n",
      "428  0  3  3  2  3  3  1\n",
      "429  0  3  3  2  3  3  2\n",
      "430  0  3  3  2  3  4  1\n",
      "431  0  3  3  2  3  4  2\n",
      "\n",
      "[601 rows x 7 columns]\n",
      "[0 1]\n",
      "     y  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15\n",
      "0    1  0  1  1  0  0  1  0  0  1   0   0   0   1   0   0\n",
      "1    1  0  0  1  0  0  1  0  0  1   0   0   0   0   0   1\n",
      "2    1  0  0  1  0  0  1  0  0  0   1   0   1   0   0   0\n",
      "3    1  0  1  1  0  0  1  0  0  0   1   0   1   0   0   0\n",
      "4    1  0  0  1  0  0  1  0  0  0   1   0   0   1   0   0\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ..  ..  ..  ..  ..  ..\n",
      "427  1  1  1  0  0  1  0  0  1  0   0   1   0   1   0   0\n",
      "428  1  1  0  0  0  1  0  0  1  0   0   1   0   0   1   0\n",
      "429  1  1  1  0  0  1  0  0  1  0   0   1   0   0   1   0\n",
      "430  1  1  0  0  0  1  0  0  1  0   0   1   0   0   0   1\n",
      "431  1  1  1  0  0  1  0  0  1  0   0   1   0   0   0   1\n",
      "\n",
      "[601 rows x 16 columns]\n",
      "    original  transformed\n",
      "0          0            1\n",
      "13         1            2\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"datasets/monk+s+problems/monks-2.train\", sep=' ', skipinitialspace=True, header=None)\n",
    "test_data = pd.read_csv(\"datasets/monk+s+problems/monks-2.test\", sep=' ', skipinitialspace=True, header=None) \n",
    "\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "\n",
    "stacked = pd.concat([train_data, test_data], ignore_index=False)\n",
    "#print(stacked)\n",
    "\n",
    "stacked = stacked.drop(columns=[7])\n",
    "\n",
    "print(stacked)\n",
    "\n",
    "\n",
    "stacked = preprocess_numerical(stacked, target_label=0)\n",
    "stacked = move_targets_to_front_and_rename(data= stacked, target_label=0)\n",
    "\n",
    "unique_values = stacked['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = stacked['y']\n",
    "\n",
    "\n",
    "stacked = make_data_binary(stacked)\n",
    "print(stacked)\n",
    "\n",
    "stacked.to_csv('datasets/monk2/monk2_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = stacked['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/monk2/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ee7db",
   "metadata": {},
   "source": [
    "monk3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd418815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1  2  3  4  5  6\n",
      "0    1  1  1  1  1  1  2\n",
      "1    1  1  1  1  1  2  1\n",
      "2    1  1  1  1  1  2  2\n",
      "3    0  1  1  1  1  3  1\n",
      "4    0  1  1  1  1  4  1\n",
      "..  .. .. .. .. .. .. ..\n",
      "427  0  3  3  2  3  2  2\n",
      "428  0  3  3  2  3  3  1\n",
      "429  0  3  3  2  3  3  2\n",
      "430  0  3  3  2  3  4  1\n",
      "431  0  3  3  2  3  4  2\n",
      "\n",
      "[554 rows x 7 columns]\n",
      "[1 0]\n",
      "     y  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15\n",
      "0    2  0  1  1  0  0  1  0  0  1   0   0   1   0   0   0\n",
      "1    2  0  0  1  0  0  1  0  0  1   0   0   0   1   0   0\n",
      "2    2  0  1  1  0  0  1  0  0  1   0   0   0   1   0   0\n",
      "3    1  0  0  1  0  0  1  0  0  1   0   0   0   0   1   0\n",
      "4    1  0  0  1  0  0  1  0  0  1   0   0   0   0   0   1\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ..  ..  ..  ..  ..  ..\n",
      "427  1  1  1  0  0  1  0  0  1  0   0   1   0   1   0   0\n",
      "428  1  1  0  0  0  1  0  0  1  0   0   1   0   0   1   0\n",
      "429  1  1  1  0  0  1  0  0  1  0   0   1   0   0   1   0\n",
      "430  1  1  0  0  0  1  0  0  1  0   0   1   0   0   0   1\n",
      "431  1  1  1  0  0  1  0  0  1  0   0   1   0   0   0   1\n",
      "\n",
      "[554 rows x 16 columns]\n",
      "   original  transformed\n",
      "0         1            2\n",
      "3         0            1\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"datasets/monk+s+problems/monks-3.train\", sep=' ', skipinitialspace=True, header=None)\n",
    "test_data = pd.read_csv(\"datasets/monk+s+problems/monks-3.test\", sep=' ', skipinitialspace=True, header=None) \n",
    "\n",
    "\n",
    "len_train_data = len(train_data)\n",
    "\n",
    "stacked = pd.concat([train_data, test_data], ignore_index=False)\n",
    "#print(stacked)\n",
    "\n",
    "stacked = stacked.drop(columns=[7])\n",
    "\n",
    "print(stacked)\n",
    "\n",
    "\n",
    "stacked = preprocess_numerical(stacked, target_label=0)\n",
    "stacked = move_targets_to_front_and_rename(data= stacked, target_label=0)\n",
    "\n",
    "unique_values = stacked['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = stacked['y']\n",
    "\n",
    "\n",
    "stacked = make_data_binary(stacked)\n",
    "print(stacked)\n",
    "\n",
    "stacked.to_csv('datasets/monk3/monk3_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = stacked['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "unique_pairs.to_csv('datasets/monk3/mapping_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14c12f",
   "metadata": {},
   "source": [
    "Microbiome taxa easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2549\n",
      "    2     3    4      5    6     7      8     9       10   11    ...  2539  \\\n",
      "1      0  48.0  1.0  22.68    0     0      0     0    4577    0  ...     0   \n",
      "2      0  51.0  1.0  23.23    0     0   1309     0  102121    0  ...     0   \n",
      "3      0  29.0  0.0  23.73    0     0    230     0    1917    0  ...    44   \n",
      "4      0  49.0  0.0  22.19    0     0   1909  3283     394    0  ...     0   \n",
      "5      0  60.0  1.0  23.53    0     0      0     0     748    0  ...    59   \n",
      "..   ...   ...  ...    ...  ...   ...    ...   ...     ...  ...  ...   ...   \n",
      "185    1  39.0  0.0   25.1    0     0      0     0   24776    0  ...   557   \n",
      "186    1  49.0  1.0  17.92    0  1610   4991     0    9524    0  ...   422   \n",
      "187    1  48.0  1.0  23.44    0     0     11     0     191    0  ...  1039   \n",
      "188    1  76.0  1.0  21.51    0     0  16325     0    1190    0  ...     0   \n",
      "189    1  42.0  0.0  22.27    0     0      0     0    3313    0  ...   331   \n",
      "\n",
      "    2540 2541 2542 2543 2544 2545 2546 2547 2548  \n",
      "1    131    0    0    0    0    0    0    0    0  \n",
      "2     47    0    0    0    0    0    0    0    0  \n",
      "3      0    0    0    0    0    0    0    0    0  \n",
      "4     28    0    0    0    0    0    0    0    0  \n",
      "5     40    0    0    0    0    0    0    0    0  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "185  135    0    0    0    0    0    0    0    0  \n",
      "186  173    0    0    0    0    0    0    0    0  \n",
      "187   54    0    0    0    0    0    0    0    0  \n",
      "188   29    0    0    0    0    0    0    0    0  \n",
      "189   14    0    0    0    0    0    0    0    0  \n",
      "\n",
      "[189 rows x 2547 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#num_rows = data.shape[0]\\n#print(num_rows)\\n\\ndata = preprocess_numerical(data, target_label=2)\\n\\ndata = move_targets_to_front_and_rename(data= data, target_label=2)\\n\\nunique_values = data['y'].unique()\\nprint(unique_values)\\n\\ntarget_vals_col_orig = data['y']\\n\\ndata = make_data_binary(data)\\nprint(data)\\n\\n# Create the directory if it doesn't exist\\nos.makedirs(f'datasets/microbiome_taxa_counts_easy', exist_ok=True)\\n\\ndata.to_csv('datasets/microbiome_taxa_counts_easy/microbiome_taxa_counts_easy_bin.csv', index=False)\\n\\ntarget_vals_col_bin = data['y']\\n\\ncombined = pd.DataFrame({\\n    'original': target_vals_col_orig,\\n    'transformed': target_vals_col_bin\\n})\\n\\nunique_pairs = combined.drop_duplicates()\\nprint(unique_pairs)\\n\\n\\nunique_pairs.to_csv('datasets/microbiome_taxa_counts_easy/mapping_transformation.csv', index=False)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/microbiome/microbiome_taxa_counts_easy.csv\", sep=',', skipinitialspace=True, header=None)\n",
    "\n",
    "data.drop(index=data.index[0], axis=0, inplace=True)\n",
    "\n",
    "#print(data)\n",
    "\n",
    "num_columns = data.shape[1]\n",
    "print(num_columns)\n",
    "\n",
    "data = data.drop(columns=[0,1])\n",
    "\n",
    "#print(data)\n",
    "\n",
    "\n",
    "#num_rows = data.shape[0]\n",
    "#print(num_rows)\n",
    "\n",
    "data = preprocess_numerical(data, target_label=2)\n",
    "\n",
    "data = move_targets_to_front_and_rename(data= data, target_label=2)\n",
    "\n",
    "unique_values = data['y'].unique()\n",
    "print(unique_values)\n",
    "\n",
    "target_vals_col_orig = data['y']\n",
    "\n",
    "data = make_data_binary(data)\n",
    "print(data)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(f'datasets/microbiome_taxa_counts_easy', exist_ok=True)\n",
    "\n",
    "#data.to_csv('datasets/microbiome_taxa_counts_easy/microbiome_taxa_counts_easy_bin.csv', index=False)\n",
    "\n",
    "target_vals_col_bin = data['y']\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'original': target_vals_col_orig,\n",
    "    'transformed': target_vals_col_bin\n",
    "})\n",
    "\n",
    "unique_pairs = combined.drop_duplicates()\n",
    "print(unique_pairs)\n",
    "\n",
    "\n",
    "#unique_pairs.to_csv('datasets/microbiome_taxa_counts_easy/mapping_transformation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
